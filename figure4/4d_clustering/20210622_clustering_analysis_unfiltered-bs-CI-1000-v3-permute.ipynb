{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing things\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.neighbors import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute the evoEnvt-ploidy 1000 times and calculate clustering metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_0_adj.csv', delimiter=','),columns=['evoEnvt-ploidy']))\n",
    "perm['random'] = np.random.permutation(perm['evoEnvt-ploidy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#making empty dataframe\n",
    "df1= pd.DataFrame()\n",
    "df2= pd.DataFrame()\n",
    "for i in range(1,1001):\n",
    "    perm = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_0_adj.csv', delimiter=','),columns=['evoEnvt-ploidy']))\n",
    "    perm['random'] = np.random.permutation(perm['evoEnvt-ploidy'])\n",
    "    #reading in data\n",
    "    gen0 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_0_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    gen200 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_200_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    gen400 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_400_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    gen600 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_600_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    gen800 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_800_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    gen1000 = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_1000_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "    cum = (pd.DataFrame(pd.read_csv('20200509_PCA_unfiltered_cum_adj.csv', delimiter=','),columns=['principal component 1','principal component 2'])).rename(columns ={\"principal component 1\":\"principle_component_1\", \"principal component 2\":\"principle_component_2\"})\n",
    "\n",
    "    #finding nearest neighbor indices\n",
    "    neigh=NearestNeighbors(n_neighbors=6) #6 neighbors is really 5, plus self, which we exclude later from analysis\n",
    "    #epoch0\n",
    "    neigh.fit(gen0)\n",
    "    gen0_neigh = (pd.DataFrame(neigh.kneighbors(gen0, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #epoch200\n",
    "    neigh.fit(gen200)\n",
    "    gen200_neigh = (pd.DataFrame(neigh.kneighbors(gen200, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #epoch400\n",
    "    neigh.fit(gen400)\n",
    "    gen400_neigh = (pd.DataFrame(neigh.kneighbors(gen400, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #epoch600\n",
    "    neigh.fit(gen600)\n",
    "    gen600_neigh = (pd.DataFrame(neigh.kneighbors(gen600, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #epoch800\n",
    "    neigh.fit(gen800)\n",
    "    gen800_neigh = (pd.DataFrame(neigh.kneighbors(gen800, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #epoch1000\n",
    "    neigh.fit(gen1000)\n",
    "    gen1000_neigh = (pd.DataFrame(neigh.kneighbors(gen1000, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "    #cumulative\n",
    "    neigh.fit(cum)\n",
    "    cum_neigh = (pd.DataFrame(neigh.kneighbors(cum, return_distance=False),columns=[\"self\",\"neigh1\", \"neigh2\", \"neigh3\", \"neigh4\", \"neigh5\"])).reset_index(drop=True)\n",
    "\n",
    "    #make new neighbors df, use dictionary and to replace neighbor index with evoEnvt\n",
    "    neighbors=['1','2','3','4','5']\n",
    "    #epoch0\n",
    "    gen0_df = pd.DataFrame()\n",
    "    random_dict = (perm).to_dict('series')\n",
    "    gen0_df['self']=gen0_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen0_df['neigh%s' % neighbor]=gen0_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # epoch200\n",
    "    gen200_df = pd.DataFrame()\n",
    "    gen200_df['self']=gen200_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen200_df['neigh%s' % neighbor]=gen200_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # epoch400\n",
    "    gen400_df = pd.DataFrame()\n",
    "    gen400_df['self']=gen400_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen400_df['neigh%s' % neighbor]=gen400_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # epoch600\n",
    "    gen600_df = pd.DataFrame()\n",
    "    gen600_df['self']=gen600_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen600_df['neigh%s' % neighbor]=gen600_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # epoch800\n",
    "    gen800_df = pd.DataFrame()\n",
    "    gen800_df['self']=gen800_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen800_df['neigh%s' % neighbor]=gen800_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # epoch1000\n",
    "    gen1000_df = pd.DataFrame()\n",
    "    gen1000_df['self']=gen1000_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        gen1000_df['neigh%s' % neighbor]=gen1000_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "    # cum\n",
    "    cum_df = pd.DataFrame()\n",
    "    cum_df['self']=cum_neigh['self'].replace(random_dict['random'])\n",
    "    for neighbor in neighbors:\n",
    "        cum_df['neigh%s' % neighbor]=cum_neigh['neigh%s' % neighbor].replace(random_dict['random'])\n",
    "\n",
    "    #counting how many neighboring nodes are from the same environment\n",
    "    def similarity(self,neigh):\n",
    "        if self == neigh:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    for neighbor in neighbors:\n",
    "        gen0_df['match%s' % neighbor] = gen0_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        gen200_df['match%s' % neighbor] = gen200_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        gen400_df['match%s' % neighbor] = gen400_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        gen600_df['match%s' % neighbor] = gen600_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        gen800_df['match%s' % neighbor] = gen800_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        gen1000_df['match%s' % neighbor] = gen1000_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "        cum_df['match%s' % neighbor] = cum_df.apply(lambda x: similarity(x['self'],x['neigh%s' % neighbor]),axis=1)\n",
    "    gen0_df['sum'] = gen0_df.sum(axis=1)\n",
    "    gen200_df['sum'] = gen200_df.sum(axis=1)\n",
    "    gen400_df['sum'] = gen400_df.sum(axis=1)\n",
    "    gen600_df['sum'] = gen600_df.sum(axis=1)\n",
    "    gen800_df['sum'] = gen800_df.sum(axis=1)\n",
    "    gen1000_df['sum'] = gen1000_df.sum(axis=1)\n",
    "    cum_df['sum'] = cum_df.sum(axis=1)\n",
    "\n",
    "    #merge dataframe with summed matches with PCs for plotting\n",
    "    gen0_plot = pd.concat([gen0,gen0_df],axis=1)\n",
    "    gen200_plot = pd.concat([gen200,gen200_df],axis=1)\n",
    "    gen400_plot = pd.concat([gen400,gen400_df],axis=1)\n",
    "    gen600_plot = pd.concat([gen600,gen600_df],axis=1)\n",
    "    gen800_plot = pd.concat([gen800,gen800_df],axis=1)\n",
    "    gen1000_plot = pd.concat([gen1000,gen1000_df],axis=1)\n",
    "    cum_plot = pd.concat([cum,cum_df],axis=1)\n",
    "\n",
    "    #average the summed matches by evolution condition\n",
    "    mean0=[]\n",
    "    mean200=[]\n",
    "    mean400=[]\n",
    "    mean600=[]\n",
    "    mean800=[]\n",
    "    mean1000=[]\n",
    "    meancum=[]\n",
    "    mean0.append(gen0_plot.groupby('self')['sum'].mean())\n",
    "    mean200.append(gen200_plot.groupby('self')['sum'].mean())\n",
    "    mean400.append(gen400_plot.groupby('self')['sum'].mean())\n",
    "    mean600.append(gen600_plot.groupby('self')['sum'].mean())\n",
    "    mean800.append(gen800_plot.groupby('self')['sum'].mean())\n",
    "    mean1000.append(gen1000_plot.groupby('self')['sum'].mean())\n",
    "    meancum.append(cum_plot.groupby('self')['sum'].mean())\n",
    "    #reformatting epoch data as x series\n",
    "    epoch = [0,200,400,600,800,1000]\n",
    "    epoch_array = np.array(epoch)\n",
    "    mean0_df=(pd.DataFrame(mean0)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    mean200_df=(pd.DataFrame(mean200)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    mean400_df=(pd.DataFrame(mean400)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    mean600_df=(pd.DataFrame(mean600)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    mean800_df=(pd.DataFrame(mean800)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    mean1000_df=(pd.DataFrame(mean1000)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "    meancum_df=(pd.DataFrame(meancum)).rename(columns={\"self\":\"\",\"YPD(37C)-D\":\"YPD37\",\"YPD(37C)-H\":\"YPD37H\",\"YPD+AA-D\":\"YPDAA\",\"YPD-D\":\"YPD\"})\n",
    "\n",
    "    true_means = pd.concat([mean0_df,mean200_df,mean400_df,mean600_df,mean800_df,mean1000_df],axis=0)\n",
    "    true_cum = pd.concat([meancum_df],axis=0)\n",
    "    true_means['gen']=epoch_array\n",
    "    df1 = pd.concat([df1,true_means])\n",
    "    df2 = pd.concat([df2,true_cum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('20210416_permute_1k_unfil.csv', index=False)\n",
    "df2.to_csv('20210416_permute_1k_cum_unfil.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
