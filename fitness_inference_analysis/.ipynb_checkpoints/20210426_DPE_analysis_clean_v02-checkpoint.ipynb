{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and set universal parameters\n",
    "\n",
    "#%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import eig\n",
    "import csv\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import scipy as stats\n",
    "from scipy import stats\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import t\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import sem\n",
    "from scipy.linalg import hadamard\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import ttest_ind_from_stats\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import OrderedDict \n",
    "#import seaborn as sns\n",
    "#sns.set()\n",
    "import sklearn\n",
    "from sklearn.neighbors import *\n",
    "import time\n",
    "from random import random\n",
    "from random import randint\n",
    "from random import randrange\n",
    "from matplotlib.lines import Line2D\n",
    "from textwrap import wrap\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statistics\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "import imageio\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import Normalize \n",
    "from scipy.interpolate import interpn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Say, \"the default sans-serif font is Arial\"\n",
    "plt.rcParams['font.sans-serif'] = \"Arial\"\n",
    "# Then, \"ALWAYS use sans-serif fonts\"\n",
    "plt.rcParams['font.family'] = \"sans-serif\"\n",
    "# Set conditions for legend\n",
    "plt.rcParams['legend.title_fontsize'] = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import cleaned data in proper format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce the keys for barcode mapping, demultiplexing as dataframes\n",
    "# First, mapping from inline index to epoch, and defining some other lists for later\n",
    "# Here \"Epoch\" refers to the generation of the evolution experiment from which populations were sampled in the bulk fitness assay (BFA)\n",
    "epochind = {'Epoch': ['0','200','400','600','800','1000'],\n",
    "           'Inline': ['GGTATCA','GATGTCCT','CTCATCCAG','GACGGAACTC','ATCGCAGGCAT','TCACGACTAGTA']}\n",
    "epochind = pd.DataFrame(epochind)\n",
    "\n",
    "epochshort = ['0','2','4','6','8','10']\n",
    "\n",
    "ploidies = ['H','D']\n",
    "envts = ['30','AA','37','RT','NaCl']\n",
    "reps = ['1','2']\n",
    "mytimepoints = ['tp1','tp3','tp5']\n",
    "\n",
    "# Note: YPD with 10% as much peptone was tried as an evolution environment originally,\n",
    "# but it was discontinued due to growth issues. Ancestors of these populations were\n",
    "# used as references in BFAs.\n",
    "\n",
    "evoenvs = ['YPD','YPD+AA','YPD(37C)','YP10%D']\n",
    "evoenvsno10 = ['YPD','YPD+AA','YPD(37C)']\n",
    "evoenvs_simp = ['30','AA','37','YP10']\n",
    "envts_abbrevofficial = ['YPD','Ac.ac.','37°C','21°C','NaCl']\n",
    "\n",
    "# purge \"unfiltered\" counts files of all errant BCs (i.e., those with the wrong inline indices)\n",
    "filenames_epoch = pd.read_csv(\"counts_filenames_take2_cat-forpipeline_v2.csv\")\n",
    "\n",
    "for countsfile in np.arange(len(filenames_epoch)):\n",
    "    # define what the inline index *should* be\n",
    "    myepochtest = epochind['Epoch'] == str(filenames_epoch['epochID'][countsfile])\n",
    "    myepochind = epochind[myepochtest]\n",
    "    relevant_inline = myepochind.iloc[:,1]\n",
    "    relevant_inline = relevant_inline.iloc[0]\n",
    "    \n",
    "    # read in one of the counts files\n",
    "    file1 = pd.read_csv(\"output1_take2_cat_regind_v2/counts/unfiltered/\"+filenames_epoch['filename'][countsfile])\n",
    "    \n",
    "    # add a column for the inline indices\n",
    "    inline_list = []\n",
    "    for mybc in np.arange(len(file1)):\n",
    "        myfind = file1['BC'][mybc].find(\"_\")\n",
    "        inline_list.append(file1['BC'][mybc][:myfind])\n",
    "    file1['Inline'] = inline_list\n",
    "    \n",
    "    # filter for only those barcodes that have the right inline index\n",
    "    inlinetest = file1['Inline'] == relevant_inline\n",
    "    file1 = file1[inlinetest]\n",
    "    \n",
    "    # export to a new folder\n",
    "    export_csv = file1.to_csv(r'output1_take2_cat_regind_v2/counts-decontam/decontam_'+filenames_epoch['filename'][countsfile],index=True,header=True)\n",
    "\n",
    "    # read in LP2 version of the counts files\n",
    "    file2 = pd.read_csv(\"output2_take2_cat_regind_v2/counts/unfiltered/\"+filenames_epoch['filename'][countsfile])\n",
    "    \n",
    "    # add a column for the inline indices\n",
    "    inline_list = []\n",
    "    for mybc in np.arange(len(file2)):\n",
    "        myfind = file2['BC'][mybc].find(\"_\")\n",
    "        inline_list.append(file2['BC'][mybc][:myfind])\n",
    "    file2['Inline'] = inline_list\n",
    "    \n",
    "    # filter for only those barcodes that have the right inline index\n",
    "    inlinetest = file2['Inline'] == relevant_inline\n",
    "    file2 = file2[inlinetest]\n",
    "    \n",
    "    # export to a new folder\n",
    "    export_csv = file2.to_csv(r'output2_take2_cat_regind_v2/counts-decontam/decontam_'+filenames_epoch['filename'][countsfile],index=True,header=True)\n",
    "    \n",
    "# make a new barcodelist\n",
    "\n",
    "# import filenames\n",
    "filenames = pd.read_csv(\"counts_filenames_take2_cat_v2.csv\")\n",
    "\n",
    "barcodes1 = pd.DataFrame()\n",
    "barcodes2 = pd.DataFrame()\n",
    "for countsfile in np.arange(len(filenames)):\n",
    "    output1 = pd.read_csv(\"output1_take2_cat_regind_v2/counts-decontam/decontam_\"+filenames['filename'][countsfile])\n",
    "    output1 = output1[['BC','Reads']]\n",
    "    barcodes1 = barcodes1.append(output1[['BC']])\n",
    "    output2 = pd.read_csv(\"output2_take2_cat_regind_v2/counts-decontam/decontam_\"+filenames['filename'][countsfile])\n",
    "    output2 = output2[['BC','Reads']]\n",
    "    barcodes2 = barcodes2.append(output2[['BC']])\n",
    "\n",
    "barcodes1 = barcodes1.drop_duplicates()\n",
    "barcodes1 = barcodes1.reset_index(drop = True)\n",
    "barcodes2 = barcodes2.drop_duplicates()\n",
    "barcodes2 = barcodes2.reset_index(drop = True)\n",
    "barcodes_all = barcodes1.append(barcodes2)\n",
    "barcodes_all = barcodes_all.drop_duplicates()\n",
    "barcodes_all = barcodes_all.reset_index(drop = True)\n",
    "\n",
    "# create a pandas dataframe that has all barcodes on the left and all read counts\n",
    "# first do LP1\n",
    "# read in the list of barcodes (expansive - larger than the actual set of barcodes)\n",
    "\n",
    "d1 = barcodes1\n",
    "\n",
    "# for each counts file, read in the info and join to the barcode list\n",
    "for countsfile in np.arange(len(filenames)):\n",
    "    d2 = pd.read_csv(\"output1_take2_cat_regind_v2/counts-decontam/decontam_\"+filenames['filename'][countsfile])\n",
    "    d2 = d2[['BC','Reads']]\n",
    "\n",
    "    abbrev = filenames['filename'][countsfile]\n",
    "    abbrev_end = abbrev.find('_S')\n",
    "    abbrev = abbrev[:abbrev_end]\n",
    "\n",
    "    d2 = d2.rename(index=str,columns={\"Reads\":abbrev+\"Reads\"})\n",
    "    d1 = pd.merge(d1,d2,on='BC',how='outer')\n",
    "\n",
    "# now do LP2\n",
    "\n",
    "d3 = barcodes2\n",
    "\n",
    "for countsfile in np.arange(len(filenames)):\n",
    "    d4 = pd.read_csv(\"output2_take2_cat_regind_v2/counts-decontam/decontam_\"+filenames['filename'][countsfile])\n",
    "    d4 = d4[['BC','Reads']]\n",
    "\n",
    "    abbrev = filenames['filename'][countsfile]\n",
    "    abbrev_end = abbrev.find('_S')\n",
    "    abbrev = abbrev[:abbrev_end]\n",
    "\n",
    "    d4 = d4.rename(index=str,columns={\"Reads\":abbrev+\"Reads\"})\n",
    "    d3 = pd.merge(d3,d4,on='BC',how='outer')\n",
    "\n",
    "LP1_Readscounts = d1\n",
    "LP2_Readscounts = d3\n",
    "\n",
    "# for each row, put in a flag about whether it came from the LP1 or LP2 parsing\n",
    "LP1_Readscounts['Parsing'] = \"LP1_parsing\"\n",
    "LP2_Readscounts['Parsing'] = \"LP2_parsing\"\n",
    "\n",
    "# append the two tables\n",
    "Readscounts = LP1_Readscounts.append(LP2_Readscounts)\n",
    "# and replace the old index for a new continuous one\n",
    "Readscounts = Readscounts.reset_index(drop=True)\n",
    "\n",
    "# parse the barcode for inline index and bc and add as columns to Readscounts\n",
    "inline_list = []\n",
    "bc_list = []\n",
    "for mybc in np.arange(len(Readscounts)):\n",
    "    myfind = Readscounts['BC'][mybc].find(\"_\")\n",
    "    inline_list.append(Readscounts['BC'][mybc][:myfind])\n",
    "    bc_list.append(Readscounts['BC'][mybc][myfind+1:])\n",
    "\n",
    "Readscounts['Inline'] = inline_list\n",
    "Readscounts['popBC'] = bc_list\n",
    "\n",
    "# merge with Readscounts\n",
    "Readscounts = pd.merge(Readscounts,epochind,on='Inline',how='left')\n",
    "\n",
    "# Second, mapping from barcode to all the salient variables, \"demographic\" info\n",
    "bcmap = pd.read_csv('20190705_bcmap_v07.csv')\n",
    "Readscounts = pd.merge(Readscounts,bcmap,on='popBC',how='left')\n",
    "\n",
    "# clean up the table\n",
    "\n",
    "# remove spurious columns\n",
    "Readscounts = Readscounts.drop('plate_id',axis=1)\n",
    "Readscounts = Readscounts.drop('row_id',axis=1)\n",
    "Readscounts = Readscounts.drop('col_id',axis=1)\n",
    "\n",
    "# want to clean up the data so there is no longer one column per epoch. Just one per tp_envt_rep.\n",
    "# Approach: Create subtables for each epoch. Then rename to the epoch-generic titles. Then append.\n",
    "\n",
    "ReadsnoE = Readscounts.copy(deep=True)\n",
    "\n",
    "consReadscounts = pd.DataFrame()\n",
    "\n",
    "for epoch in np.arange(len(epochshort)):\n",
    "    mynew = ReadsnoE.loc[:,['BC','tp0_na-na_e'+epochshort[epoch]+'Reads','tp1_30-1_e'+epochshort[epoch]+'Reads','tp3_30-1_e'+epochshort[epoch]+'Reads','tp5_30-1_e'+epochshort[epoch]+'Reads','tp1_30-2_e'+epochshort[epoch]+'Reads','tp3_30-2_e'+epochshort[epoch]+'Reads','tp5_30-2_e'+epochshort[epoch]+'Reads','tp1_37-1_e'+epochshort[epoch]+'Reads','tp3_37-1_e'+epochshort[epoch]+'Reads','tp5_37-1_e'+epochshort[epoch]+'Reads','tp1_37-2_e'+epochshort[epoch]+'Reads','tp3_37-2_e'+epochshort[epoch]+'Reads','tp5_37-2_e'+epochshort[epoch]+'Reads','tp1_AA-1_e'+epochshort[epoch]+'Reads','tp3_AA-1_e'+epochshort[epoch]+'Reads','tp5_AA-1_e'+epochshort[epoch]+'Reads','tp1_AA-2_e'+epochshort[epoch]+'Reads','tp3_AA-2_e'+epochshort[epoch]+'Reads','tp5_AA-2_e'+epochshort[epoch]+'Reads','tp1_RT-1_e'+epochshort[epoch]+'Reads','tp3_RT-1_e'+epochshort[epoch]+'Reads','tp5_RT-1_e'+epochshort[epoch]+'Reads','tp1_RT-2_e'+epochshort[epoch]+'Reads','tp3_RT-2_e'+epochshort[epoch]+'Reads','tp5_RT-2_e'+epochshort[epoch]+'Reads','tp1_NaCl-1_e'+epochshort[epoch]+'Reads','tp3_NaCl-1_e'+epochshort[epoch]+'Reads','tp5_NaCl-1_e'+epochshort[epoch]+'Reads','tp1_NaCl-2_e'+epochshort[epoch]+'Reads','tp3_NaCl-2_e'+epochshort[epoch]+'Reads','tp5_NaCl-2_e'+epochshort[epoch]+'Reads','Parsing','Inline','popBC','Epoch','dip_bc','evocode','ploidy','evoEnvt','celltype','LP','BCs-in-pop']]\n",
    "    mytest = mynew['Epoch'] == epochind['Epoch'][epoch]\n",
    "    mynew2 = mynew[mytest]\n",
    "    mynew2 = mynew2.rename(columns={'tp0_na-na_e'+epochshort[epoch]+'Reads':'tp0_na-na_Reads','tp1_30-1_e'+epochshort[epoch]+'Reads':'tp1_30-1_Reads','tp3_30-1_e'+epochshort[epoch]+'Reads':'tp3_30-1_Reads','tp5_30-1_e'+epochshort[epoch]+'Reads':'tp5_30-1_Reads','tp1_30-2_e'+epochshort[epoch]+'Reads':'tp1_30-2_Reads','tp3_30-2_e'+epochshort[epoch]+'Reads':'tp3_30-2_Reads','tp5_30-2_e'+epochshort[epoch]+'Reads':'tp5_30-2_Reads','tp1_37-1_e'+epochshort[epoch]+'Reads':'tp1_37-1_Reads','tp3_37-1_e'+epochshort[epoch]+'Reads':'tp3_37-1_Reads','tp5_37-1_e'+epochshort[epoch]+'Reads':'tp5_37-1_Reads','tp1_37-2_e'+epochshort[epoch]+'Reads':'tp1_37-2_Reads','tp3_37-2_e'+epochshort[epoch]+'Reads':'tp3_37-2_Reads','tp5_37-2_e'+epochshort[epoch]+'Reads':'tp5_37-2_Reads','tp1_AA-1_e'+epochshort[epoch]+'Reads':'tp1_AA-1_Reads','tp3_AA-1_e'+epochshort[epoch]+'Reads':'tp3_AA-1_Reads','tp5_AA-1_e'+epochshort[epoch]+'Reads':'tp5_AA-1_Reads','tp1_AA-2_e'+epochshort[epoch]+'Reads':'tp1_AA-2_Reads','tp3_AA-2_e'+epochshort[epoch]+'Reads':'tp3_AA-2_Reads','tp5_AA-2_e'+epochshort[epoch]+'Reads':'tp5_AA-2_Reads','tp1_RT-1_e'+epochshort[epoch]+'Reads':'tp1_RT-1_Reads','tp3_RT-1_e'+epochshort[epoch]+'Reads':'tp3_RT-1_Reads','tp5_RT-1_e'+epochshort[epoch]+'Reads':'tp5_RT-1_Reads','tp1_RT-2_e'+epochshort[epoch]+'Reads':'tp1_RT-2_Reads','tp3_RT-2_e'+epochshort[epoch]+'Reads':'tp3_RT-2_Reads','tp5_RT-2_e'+epochshort[epoch]+'Reads':'tp5_RT-2_Reads','tp1_NaCl-1_e'+epochshort[epoch]+'Reads':'tp1_NaCl-1_Reads','tp3_NaCl-1_e'+epochshort[epoch]+'Reads':'tp3_NaCl-1_Reads','tp5_NaCl-1_e'+epochshort[epoch]+'Reads':'tp5_NaCl-1_Reads','tp1_NaCl-2_e'+epochshort[epoch]+'Reads':'tp1_NaCl-2_Reads','tp3_NaCl-2_e'+epochshort[epoch]+'Reads':'tp3_NaCl-2_Reads','tp5_NaCl-2_e'+epochshort[epoch]+'Reads':'tp5_NaCl-2_Reads'})\n",
    "    consReadscounts = consReadscounts.append(mynew2)\n",
    "\n",
    "consReadscounts = consReadscounts.reset_index(drop=True)    \n",
    "\n",
    "# But I want these data to be better and ready for s inference.\n",
    "# To do that, want to sum the barcodes for a given population\n",
    "\n",
    "# Add a column for evocode-epoch combinations\n",
    "consReadscounts['evocode-Epoch'] = consReadscounts['evocode']+\"-\"+consReadscounts['Epoch'].map(str)\n",
    "\n",
    "# Create a reference list containing all evocode-Epochs\n",
    "eEs = consReadscounts['evocode-Epoch'].drop_duplicates()\n",
    "# get rid of the nan values for the barcodes we don't have associated with pops yet!\n",
    "eEs = eEs.dropna()\n",
    "\n",
    "eEs = eEs.values.tolist()\n",
    "\n",
    "# go through and sum up the barcodes for each evocode-Epoch\n",
    "# create an empty dataframe that will serve as the ultimate repository\n",
    "summedReads = pd.DataFrame()\n",
    "\n",
    "for e in np.arange(len(eEs)):\n",
    "    # create an empty DataFrame that will house the new row\n",
    "    newrow = pd.DataFrame()\n",
    "    # create subtable for a given evocode-Epoch\n",
    "    mytest = consReadscounts['evocode-Epoch'] == eEs[e]\n",
    "    mytesttoys1 = consReadscounts[mytest]\n",
    "    # populate newrow with the qualitative info\n",
    "    newrow['evocode-Epoch'] = mytesttoys1['evocode-Epoch']\n",
    "    newrow['Parsing'] = mytesttoys1['Parsing']\n",
    "    newrow['Epoch'] = mytesttoys1['Epoch']\n",
    "    newrow['evocode'] = mytesttoys1['evocode']\n",
    "    newrow['ploidy'] = mytesttoys1['ploidy']\n",
    "    newrow['evoEnvt'] = mytesttoys1['evoEnvt']\n",
    "    newrow['celltype'] = mytesttoys1['celltype']\n",
    "    newrow['LP'] = mytesttoys1['LP']\n",
    "    newrow['BCs-in-pop'] = mytesttoys1['BCs-in-pop']\n",
    "    newrow = newrow.iloc[0,:]\n",
    "    newrow = newrow.to_frame()\n",
    "    newrow = newrow.transpose()\n",
    "    \n",
    "    # add sums of other fields\n",
    "    for countsfile in np.arange(len(filenames)):\n",
    "        abbrev = filenames['filename'][countsfile]\n",
    "        abbrev_end = abbrev.find('_e')\n",
    "        abbrev = abbrev[:abbrev_end]\n",
    "        abbrev = abbrev+'_Reads'\n",
    "        newrow[abbrev] = mytesttoys1[abbrev].sum()\n",
    "    \n",
    "    # append this new row to the summedReads dataframe\n",
    "    summedReads = summedReads.append(newrow)\n",
    "\n",
    "\n",
    "# add one set of rows specifically for those not assigned an evocode\n",
    "newrow = pd.DataFrame()\n",
    "# create subtable for a given evocode-Epoch\n",
    "mytest = consReadscounts['evocode'].isnull()\n",
    "mytesttoys1 = consReadscounts[mytest]\n",
    "# populate newrow with the qualitative info\n",
    "newrow['evocode-Epoch'] = mytesttoys1['evocode-Epoch']\n",
    "newrow['evocode'] = mytesttoys1['evocode']\n",
    "newrow['ploidy'] = mytesttoys1['ploidy']\n",
    "newrow['evoEnvt'] = mytesttoys1['evoEnvt']\n",
    "newrow['celltype'] = mytesttoys1['celltype']\n",
    "newrow['LP'] = mytesttoys1['LP']\n",
    "newrow['BCs-in-pop'] = mytesttoys1['BCs-in-pop']\n",
    "newrow = newrow.iloc[0,:]\n",
    "newrow = newrow.to_frame()\n",
    "newrow = newrow.transpose()\n",
    "\n",
    "# add sums of other fields\n",
    "for countsfile in np.arange(len(filenames)):\n",
    "    abbrev = filenames['filename'][countsfile]\n",
    "    abbrev_end = abbrev.find('_e')\n",
    "    abbrev = abbrev[:abbrev_end]\n",
    "    abbrev = abbrev+'_Reads'\n",
    "    newrow[abbrev] = mytesttoys1[abbrev].sum()\n",
    "\n",
    "# append this new row to the summedReads dataframe\n",
    "summedReads = summedReads.append(newrow)\n",
    "\n",
    "#reset index\n",
    "summedReads = summedReads.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create new table for these calculations.\n",
    "logtable = summedReads.copy(deep=True)\n",
    "\n",
    "# Second, create and populate new fields with log frequencies\n",
    "for ploidy in np.arange(0,2):\n",
    "    for epoch in np.arange(len(epochind['Epoch'])):\n",
    "        for LP in np.arange(1,3):\n",
    "            for envt in np.arange(len(envts)):\n",
    "                for rep in np.arange(len(reps)):\n",
    "                    mytesttoys4 = logtable.loc[(logtable['ploidy'] == ploidies[ploidy])&(logtable['Epoch'] == epochind['Epoch'][epoch])&(logtable['LP'] == LP)].copy(deep=True).reset_index(drop=True)\n",
    "                    mytesttoys4 = mytesttoys4[['evocode-Epoch','tp1_'+envts[envt]+'-'+reps[rep]+'_Reads','tp3_'+envts[envt]+'-'+reps[rep]+'_Reads','tp5_'+envts[envt]+'-'+reps[rep]+'_Reads']]\n",
    "                    tp1sum = np.nansum(mytesttoys4.loc[:,'tp1_'+envts[envt]+'-'+reps[rep]+'_Reads'])\n",
    "                    tp3sum = np.nansum(mytesttoys4.loc[:,'tp3_'+envts[envt]+'-'+reps[rep]+'_Reads'])\n",
    "                    tp5sum = np.nansum(mytesttoys4.loc[:,'tp5_'+envts[envt]+'-'+reps[rep]+'_Reads'])\n",
    "                    \n",
    "                    if tp1sum > 0:\n",
    "                        mytesttoys4['tp1freq'] = mytesttoys4['tp1_'+envts[envt]+'-'+reps[rep]+'_Reads']/tp1sum\n",
    "                    else:\n",
    "                        mytesttoys4['tp1freq'] = np.nan\n",
    "\n",
    "                    if tp3sum > 0:\n",
    "                        mytesttoys4['tp3freq'] = mytesttoys4['tp3_'+envts[envt]+'-'+reps[rep]+'_Reads']/tp3sum\n",
    "                    else:\n",
    "                        mytesttoys4['tp3freq'] = np.nan\n",
    "\n",
    "                    if tp5sum > 0:\n",
    "                        mytesttoys4['tp5freq'] = mytesttoys4['tp5_'+envts[envt]+'-'+reps[rep]+'_Reads']/tp5sum\n",
    "                    else:\n",
    "                        mytesttoys4['tp5freq'] = np.nan\n",
    "                    \n",
    "                    mytesttoys4['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp1__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP)] = np.log(mytesttoys4['tp1freq'])\n",
    "                    mytesttoys4['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp3__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP)] = np.log(mytesttoys4['tp3freq'])\n",
    "                    mytesttoys4['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp5__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP)] = np.log(mytesttoys4['tp5freq'])\n",
    "                    \n",
    "                    addlogfreqs = mytesttoys4[['evocode-Epoch','logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp1__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP),'logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp3__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP),'logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp5__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP)]]\n",
    "                    logtable = pd.merge(logtable,addlogfreqs,on='evocode-Epoch',how='left')\n",
    "\n",
    "# now have to consolidate logtable - super ungainly with lots of different columns.\n",
    "# Clean it up.\n",
    "logtable2 = logtable.copy(deep=True)\n",
    "conslogtable = pd.DataFrame()\n",
    "for ploidy in np.arange(0,2):\n",
    "    for epoch in np.arange(len(epochind['Epoch'])):\n",
    "        for LP in np.arange(1,3):\n",
    "            mytesttoys3 = logtable2.loc[(logtable2['ploidy'] == ploidies[ploidy])&(logtable2['Epoch'] == epochind['Epoch'][epoch])&(logtable2['LP'] == LP)].copy(deep=True).reset_index(drop=True)\n",
    "            mytesttoys4 = mytesttoys3.iloc[:,0:40]\n",
    "            for envt in np.arange(len(envts)):\n",
    "                for rep in np.arange(len(reps)):\n",
    "                    for tp in np.arange(len(mytimepoints)):\n",
    "                        mytesttoys4['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-'+mytimepoints[tp]] = mytesttoys3['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-'+mytimepoints[tp]+'__'+ploidies[ploidy]+'-'+epochind['Epoch'][epoch]+'-'+str(LP)]\n",
    "            conslogtable = conslogtable.append(mytesttoys4)\n",
    "\n",
    "conslogtable = conslogtable.reset_index(drop=True)\n",
    "\n",
    "# change conslogtable so all the -inf values (corresponding to attempts to take the log of 0)\n",
    "# are replaced with np.nan.\n",
    "conslogtable = conslogtable.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Now to get the logfreq slopes that I'll use to get the s values\n",
    "for envt in np.arange(len(envts)):\n",
    "    for rep in np.arange(len(reps)):\n",
    "        conslogtable['10-30slope_'+envts[envt]+'-rep'+reps[rep]] = (conslogtable['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp3']-conslogtable['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp1'])/20\n",
    "        conslogtable['30-50slope_'+envts[envt]+'-rep'+reps[rep]] = (conslogtable['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp5']-conslogtable['logfreq_'+envts[envt]+'-rep'+reps[rep]+'-tp3'])/20\n",
    "\n",
    "# Subtract out the median of the reference logfreq slopes to get my s values\n",
    "# references to use for each ploidy and LP\n",
    "refsH1 = ['p1-E11','p1-F10']\n",
    "refsH2 = ['p1-B11','p1-C10','p1-D9']\n",
    "refsD1 = ['p1-B5', 'p1-C7', 'p1-E8', 'p1-F7']\n",
    "refsD2 = ['p1-B8', 'p1-E5', 'p1-H5', 'p1-H8', 'p2-B5']\n",
    "\n",
    "allrefs = refsH1 + refsH2 + refsD1 + refsD2\n",
    "\n",
    "# extract all these values into a table of their own\n",
    "refstable = conslogtable.loc[conslogtable['evocode'].isin(allrefs)]\n",
    "refstable = refstable.reset_index(drop=True)\n",
    "\n",
    "# Set a threshold for reference counts\n",
    "count_thresh = 5\n",
    "\n",
    "# make these new s values in a new table\n",
    "conslogtable_withs1= pd.DataFrame()\n",
    "\n",
    "for ploidy in np.arange(0,2):\n",
    "    for LP in np.arange(1,3):\n",
    "        for epoch in np.arange(len(epochind['Epoch'])):\n",
    "            # generate table of relevant references\n",
    "            rel_refs = refstable.loc[(refstable['Epoch'] == epochind['Epoch'][epoch]) & (refstable['ploidy'] == ploidies[ploidy]) & (refstable['LP'] == LP)].copy(deep=True)\n",
    "            rel_refs = rel_refs.reset_index(drop=True)\n",
    "            # generate the appropriate subtable for experimental strains\n",
    "            mytesttoys3 = conslogtable.loc[(conslogtable['Epoch'] == epochind['Epoch'][epoch]) & (conslogtable['ploidy'] == ploidies[ploidy]) & (conslogtable['LP'] == LP)].copy(deep=True)\n",
    "            mytesttoys3 = mytesttoys3.reset_index(drop=True)\n",
    "            for envt in np.arange(len(envts)):\n",
    "                for rep in np.arange(len(reps)):\n",
    "                    # calculate these interval-specific s values if and only if the reference counts used in the calculation have an average of greater than count_thresh\n",
    "                    if np.nanmean(rel_refs['tp1_'+envts[envt]+'-'+reps[rep]+'_Reads']) > count_thresh and np.nanmean(rel_refs['tp3_'+envts[envt]+'-'+reps[rep]+'_Reads']) > count_thresh:\n",
    "                        mytesttoys3['s_10-30_'+envts[envt]+'-rep'+reps[rep]] = mytesttoys3['10-30slope_'+envts[envt]+'-rep'+reps[rep]] - np.nanmedian(rel_refs['10-30slope_'+envts[envt]+'-rep'+reps[rep]])\n",
    "                    else:\n",
    "                        mytesttoys3['s_10-30_'+envts[envt]+'-rep'+reps[rep]] = np.nan\n",
    "                    if np.nanmean(rel_refs['tp3_'+envts[envt]+'-'+reps[rep]+'_Reads']) > count_thresh and np.nanmean(rel_refs['tp5_'+envts[envt]+'-'+reps[rep]+'_Reads']) > count_thresh:\n",
    "                        mytesttoys3['s_30-50_'+envts[envt]+'-rep'+reps[rep]] = mytesttoys3['30-50slope_'+envts[envt]+'-rep'+reps[rep]] - np.nanmedian(rel_refs['30-50slope_'+envts[envt]+'-rep'+reps[rep]])\n",
    "                    else:\n",
    "                        mytesttoys3['s_30-50_'+envts[envt]+'-rep'+reps[rep]] = np.nan\n",
    "                        \n",
    "            conslogtable_withs1 = conslogtable_withs1.append(mytesttoys3)\n",
    "conslogtable_withs1 = conslogtable_withs1.reset_index(drop=True)\n",
    "\n",
    "conslogtable_withs1['Epoch'] = conslogtable_withs1['Epoch'].astype(float)\n",
    "\n",
    "# Now average the interval-specific s values to find the replicate-specific s value\n",
    "# And average the replicate-specific s values to get the final s value (and get some error)\n",
    "myints = ['10-30','30-50']\n",
    "for i in np.arange(len(conslogtable_withs1)):\n",
    "    for e in np.arange(len(envts)):\n",
    "        for r in np.arange(1,3):\n",
    "            worklist = []\n",
    "            for v in np.arange(2):\n",
    "                if conslogtable_withs1.loc[i,'s_'+myints[v]+'_'+envts[e]+'-rep'+str(r)] != np.nan:\n",
    "                    worklist = worklist + [conslogtable_withs1.loc[i,'s_'+myints[v]+'_'+envts[e]+'-rep'+str(r)]]\n",
    "            conslogtable_withs1.at[i,'s_'+envts[e]+'-rep'+str(r)] = np.nanmean(worklist)\n",
    "        conslogtable_withs1.at[i,'s_'+envts[e]] = np.nanmean([conslogtable_withs1.loc[i,'s_'+envts[e]+'-rep1'],conslogtable_withs1.loc[i,'s_'+envts[e]+'-rep2']])\n",
    "        conslogtable_withs1.at[i,'stderr(s)_'+envts[e]] = stats.sem([conslogtable_withs1.loc[i,'s_'+envts[e]+'-rep1'],conslogtable_withs1.loc[i,'s_'+envts[e]+'-rep2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove outliers (or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the flag for whether we are removing outliers or not\n",
    "removingoutliers = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "milostable = conslogtable_withs1.copy(deep=True)\n",
    "\n",
    "e0table = milostable.loc[milostable['Epoch'] == 0]\n",
    "\n",
    "if removingoutliers == True:\n",
    "    diff_thresh = 0.04\n",
    "else:\n",
    "    diff_thresh = 1\n",
    "\n",
    "# generate a table of the relevant stats for epoch 0 for each LP, envt, ploidy\n",
    "# at the same time, generate a list of outliers at the diff_thresh\n",
    "\n",
    "ploidytable = []\n",
    "LPtable = []\n",
    "envttable = []\n",
    "median_s = []\n",
    "mean_s = []\n",
    "std_s = []\n",
    "my_evocodes = []\n",
    "outlier_pops = []\n",
    "outlier_ploidies = []\n",
    "\n",
    "for ploidy in np.arange(len(ploidies)):\n",
    "    for LP in np.arange(1,3):\n",
    "        e0subtable = e0table.loc[(e0table['ploidy'] == ploidies[ploidy]) & (e0table['LP'] == LP)]\n",
    "        e0subtable = e0subtable.reset_index(drop=True)\n",
    "        for envt in np.arange(len(envts)):\n",
    "            ploidytable = ploidytable + [ploidies[ploidy]]\n",
    "            LPtable = LPtable + [LP]\n",
    "            envttable = envttable + [envts[envt]]\n",
    "            median_s = median_s + [np.nanmedian(e0subtable['s_'+envts[envt]])]\n",
    "            std_s = std_s + [np.std(e0subtable['s_'+envts[envt]])]\n",
    "            mean_s = mean_s + [np.nanmean(e0subtable['s_'+envts[envt]])]\n",
    "            for e in np.arange(len(e0subtable)):\n",
    "                if e0subtable.loc[e,'s_'+envts[envt]] < np.nanmedian(e0subtable['s_'+envts[envt]]) - diff_thresh or e0subtable.loc[e,'s_'+envts[envt]] > np.nanmedian(e0subtable['s_'+envts[envt]]) + diff_thresh:\n",
    "                    outlier_pops = outlier_pops + [e0subtable.loc[e,'evocode']]\n",
    "                    outlier_ploidies = outlier_ploidies + [e0subtable.loc[e,'ploidy']]\n",
    "                \n",
    "e0table_stats = pd.DataFrame()\n",
    "e0table_stats['ploidy'] = ploidytable\n",
    "e0table_stats['LP'] = LPtable\n",
    "e0table_stats['assayEnvt'] = envttable\n",
    "e0table_stats['mean_e0_s'] = mean_s\n",
    "e0table_stats['median_e0_s'] = median_s\n",
    "e0table_stats['std_e0_s'] = std_s\n",
    "\n",
    "outlier_pops = list(OrderedDict.fromkeys(outlier_pops))\n",
    "\n",
    "# map this list of outliers to milostable\n",
    "outliertracker = []\n",
    "\n",
    "for i in np.arange(len(milostable)):\n",
    "    if milostable.loc[i,'evocode'] in outlier_pops:\n",
    "        outliertracker = outliertracker + ['y']\n",
    "    else:\n",
    "        outliertracker = outliertracker + ['n']\n",
    "\n",
    "milostable['outlier'] = outliertracker\n",
    "out1 = outlier_pops\n",
    "#print(str(len(outlier_pops)))\n",
    "\n",
    "# Now, create a version of the table with just demographic info, s estimates, and standard errors\n",
    "finaltable = milostable.copy(deep = True)\n",
    "finaltable = finaltable.loc[(finaltable['outlier'] != 'y')].reset_index(drop=True)\n",
    "new_evocode_list = finaltable[['evocode','LP','ploidy','evoEnvt']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Dropped p2 on gen 180 day, went back and thawed from 150. So \"200\" => actually 160 for p2, etc.\n",
    "finaltable['Epoch-true'] = \"\"\n",
    "for ece in np.arange(len(finaltable)):\n",
    "    if finaltable.loc[ece,'evocode'][1] == '2' and finaltable.loc[ece,'Epoch'] > 0:\n",
    "        finaltable.at[ece,'Epoch-true'] = finaltable.loc[ece,'Epoch'] - 40\n",
    "    else:\n",
    "        finaltable.at[ece,'Epoch-true'] = finaltable.loc[ece,'Epoch']\n",
    "    \n",
    "# Also, for ease of plotting/calculation later, we add columns that contain an interpolated/extrapolated fitness\n",
    "# corresponding to each epoch for the p2 populations.\n",
    "for envt in np.arange(len(envts)):\n",
    "    finaltable['s_'+envts[envt]+'_adj'] = \"\"\n",
    "\n",
    "for envt in np.arange(len(envts)):\n",
    "    for ece in np.arange(len(finaltable)):\n",
    "        if finaltable.loc[ece,'evocode'][1] == '2' and finaltable.loc[ece,'Epoch'] > 0:\n",
    "            if finaltable.loc[ece,'Epoch-true'] < 960:\n",
    "                this_s = finaltable.loc[ece,'s_'+envts[envt]]\n",
    "                next_s = finaltable.loc[(finaltable['evocode'] == finaltable.loc[ece,'evocode']) &\n",
    "                                        (finaltable['Epoch-true'] == finaltable.loc[ece,'Epoch-true'] + 200)]['s_'+envts[envt]].iloc[0]\n",
    "                my_m = (next_s - this_s)/200\n",
    "                my_b = this_s - finaltable.loc[ece,'Epoch-true'] * my_m\n",
    "                new_s = my_m * finaltable.loc[ece,'Epoch'] + my_b\n",
    "                finaltable.at[ece,'s_'+envts[envt]+'_adj'] = new_s\n",
    "            else:\n",
    "                this_s = finaltable.loc[ece,'s_'+envts[envt]]\n",
    "                prev_s = finaltable.loc[(finaltable['evocode'] == finaltable.loc[ece,'evocode']) &\n",
    "                                        (finaltable['Epoch-true'] == finaltable.loc[ece,'Epoch-true'] - 200)]['s_'+envts[envt]].iloc[0]\n",
    "                my_m = (this_s - prev_s)/200\n",
    "                my_b = this_s - finaltable.loc[ece,'Epoch-true'] * my_m\n",
    "                new_s = my_m * finaltable.loc[ece,'Epoch'] + my_b\n",
    "                finaltable.at[ece,'s_'+envts[envt]+'_adj'] = new_s                \n",
    "        else:\n",
    "            finaltable.at[ece,'s_'+envts[envt]+'_adj'] = finaltable.loc[ece,'s_'+envts[envt]]\n",
    "\n",
    "if removingoutliers == True:\n",
    "    export_csv = finaltable.to_csv(r'20210413_final_s_table_threshold.csv',index=True,header=True)\n",
    "else:\n",
    "    export_csv = finaltable.to_csv(r'20210413_final_s_table_unfiltered.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1–figure supplement 1 (technical replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare technical replicates, density plot style\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(4*0.8,5.2*0.8), sharex=False, sharey=False, constrained_layout=True)\n",
    "\n",
    "axlist = [[0,0],[1,0],[2,0],[0,1],[1,1]]\n",
    "\n",
    "#for e in np.arange(0,1):\n",
    "for e in np.arange(len(envts)):\n",
    "    \n",
    "    x = finaltable['s_'+envts[e]+'-rep1']\n",
    "    y = finaltable['s_'+envts[e]+'-rep2']\n",
    "\n",
    "    mask = ~np.isnan(x) & ~np.isnan(y)\n",
    "    x = x[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "\n",
    "    # Find distance to all other points\n",
    "    nei = pd.DataFrame()\n",
    "    nei['x'] = x\n",
    "    nei['y'] = y\n",
    "\n",
    "    for i in np.arange(len(nei)):\n",
    "        focal_x = nei.loc[i,'x']\n",
    "        focal_y = nei.loc[i,'y']\n",
    "        temp = pd.DataFrame()\n",
    "        temp['dist'] = np.sqrt((focal_x-nei['x'])**2+(focal_y-nei['y'])**2)\n",
    "        temp.at[i,'dist'] = np.nan\n",
    "        # Get average distance to 5 nearest neighbors\n",
    "        nei.at[i,'dist'] = temp.sort_values(by='dist',ascending=True).reset_index(drop=True)[:5].mean().values[0]\n",
    "\n",
    "    nei = nei.sort_values(by='dist',ascending=False).reset_index(drop=True)\n",
    "\n",
    "    minima=min(nei['dist'])\n",
    "    maxima=max(nei['dist'])\n",
    "    mynorm = mcolors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "    mapper = cm.ScalarMappable(norm=mynorm, cmap='viridis_r')\n",
    "\n",
    "    x = nei['x']\n",
    "    y = nei['y']\n",
    "\n",
    "    normvals = MinMaxScaler().fit_transform(np.array(nei['dist']).reshape(-1,1)).tolist()\n",
    "    nv = []\n",
    "    for i in np.arange(len(normvals)):\n",
    "        nv = nv + [normvals[i][0]]\n",
    "\n",
    "    mycs = []\n",
    "    for v in nv:\n",
    "        mycs = mycs + [mapper.to_rgba(v)]\n",
    "    \n",
    "    for i in np.arange(len(x)):\n",
    "        axes[axlist[e][0]][axlist[e][1]].scatter(x[i],y[i],c=np.array([mycs[i]]),\n",
    "                                                 s=5,alpha=0.7)\n",
    "  \n",
    "    \n",
    "    mymin = min(finaltable['s_'+envts[e]+'-rep1'].min(),finaltable['s_'+envts[e]+'-rep2'].min())\n",
    "    mymax = max(finaltable['s_'+envts[e]+'-rep1'].max(),finaltable['s_'+envts[e]+'-rep2'].max())\n",
    "    mydiff = mymax - mymin\n",
    "    myspace = 0.05\n",
    "    \n",
    "    axes[axlist[e][0]][axlist[e][1]].plot(np.linspace(-1,1),np.linspace(-1,1),color='k',lw=0.5,zorder=0)\n",
    "    \n",
    "    axes[axlist[e][0]][axlist[e][1]].set_xlim(mymin-myspace*mydiff,mymax+myspace*mydiff)\n",
    "    axes[axlist[e][0]][axlist[e][1]].set_ylim(mymin-myspace*mydiff,mymax+myspace*mydiff)\n",
    "    \n",
    "    reg = linregress(finaltable['s_'+envts[e]+'-rep1'],finaltable['s_'+envts[e]+'-rep2'])\n",
    "    #print('R^2 = '+str(reg.rvalue**2))\n",
    "    #print('********')\n",
    "    \n",
    "    axes[axlist[e][0]][axlist[e][1]].text(mymax-mydiff*0.97,mymax-mydiff*0.01,envts_abbrevofficial[e],\n",
    "               verticalalignment='top',horizontalalignment='left',fontsize=8)\n",
    "    \n",
    "    plt.rcParams['mathtext.default']='regular'\n",
    "    axes[axlist[e][0]][axlist[e][1]].text(mymax-mydiff*-0.03,mymax-mydiff*0.84,'$R^{2}=$'+str(round(reg.rvalue**2,3)),\n",
    "               verticalalignment='top',horizontalalignment='right',fontsize=8)\n",
    "    \n",
    "    axes[axlist[e][0]][axlist[e][1]].tick_params(axis='both',labelsize=7)\n",
    "\n",
    "fig.delaxes(axes[2][1])\n",
    "\n",
    "#if removingoutliers == False:\n",
    "#    plt.savefig('techrepsmultipanel_v01.pdf',bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(7,5.6), sharex=True, sharey='row', constrained_layout=True)\n",
    "\n",
    "fig.suptitle('Evolution environment',x=.5,y=1.03,size=10, fontweight='bold')\n",
    "fig.text(0.5, -0.02, 'Generation', ha='center',size=8, fontweight = 'bold')\n",
    "fig.text(-0.03, 0.5, 'Fitness in assay environment', va='center', rotation='vertical',size=10, fontweight = 'bold')\n",
    "\n",
    "rows = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "cols = ['YPD','YPD + Acetic acid','YPD, 37°C (diploid)','YPD, 37°C (haploid)']\n",
    "\n",
    "for ax, col in zip(axes[0], cols):\n",
    "    ax.set_title(col, size=8,fontweight='bold')\n",
    "\n",
    "for ax, row in zip(axes[:,0], rows):\n",
    "    ax.set_ylabel(row, rotation=90, size=8,fontweight='bold')\n",
    "    \n",
    "for ploidy in np.arange(2):\n",
    "    ploidytest = finaltable['ploidy'] == ploidies[ploidy]\n",
    "    ploidysubtable = finaltable[ploidytest]\n",
    "    for ee in np.arange(len(evoenvsno10)):\n",
    "        evoenvtest = ploidysubtable['evoEnvt'] == evoenvsno10[ee]\n",
    "        ploidyeetable = ploidysubtable[evoenvtest]\n",
    "        \n",
    "        if len(ploidyeetable) > 0:\n",
    "            # make a list of all the represented evocodes\n",
    "            temptest1 = new_evocode_list['ploidy'] == ploidies[ploidy]\n",
    "            temp_evocodes1 = new_evocode_list[temptest1]\n",
    "            temptest2 = temp_evocodes1['evoEnvt'] == evoenvsno10[ee]\n",
    "            temp_evocodes = temp_evocodes1[temptest2]\n",
    "            temp_evocodes = temp_evocodes.drop_duplicates()\n",
    "            temp_evocodes = temp_evocodes.reset_index(drop=True)\n",
    "\n",
    "            # for each of the assay environments, plot lines for all the represented evocodes\n",
    "            for assay_env in np.arange(len(envts)):\n",
    "                for ec in np.arange(len(temp_evocodes)):\n",
    "                    ectest = ploidyeetable['evocode'] == temp_evocodes['evocode'][ec]\n",
    "                    ecsubtable = ploidyeetable[ectest]\n",
    "                    ecsubtable = ecsubtable.sort_values(by=['Epoch'])\n",
    "                    if ploidy == 1:\n",
    "                        axes[assay_env,ee].errorbar(ecsubtable['Epoch-true'],ecsubtable['s_'+envts[assay_env]],xerr=None,yerr=ecsubtable['stderr(s)_'+envts[assay_env]],linewidth=0.5,elinewidth=0.3,color='xkcd:grey')\n",
    "                    elif ploidy == 0:\n",
    "                        axes[assay_env,ee+1].errorbar(ecsubtable['Epoch-true'],ecsubtable['s_'+envts[assay_env]],xerr=None,yerr=ecsubtable['stderr(s)_'+envts[assay_env]],linewidth=0.5,elinewidth=0.3,color='xkcd:grey')\n",
    "                    \n",
    "                meantable = pd.DataFrame()\n",
    "                meantable['Epoch'] = ecsubtable['Epoch']\n",
    "                meantable['median_s'] = \"\"\n",
    "                meantable = meantable.reset_index(drop=True)\n",
    "\n",
    "                for epoch in np.arange(len(epochind)):\n",
    "                    tomakemean = ploidyeetable.loc[(ploidyeetable['Epoch'] == float(epochind['Epoch'][epoch]))]\n",
    "                    if len(tomakemean) > 0:\n",
    "                        meantable.at[epoch,'median_s'] = np.median(tomakemean['s_'+envts[assay_env]+'_adj'])\n",
    "                if len(tomakemean) > 0:\n",
    "                    if ploidy == 1:\n",
    "                        axes[assay_env,ee].errorbar(meantable['Epoch'],meantable['median_s'],xerr=None,yerr=None,linewidth=1,color='k')\n",
    "                        axes[assay_env][ee].tick_params(axis='both',labelsize=7)\n",
    "                    elif ploidy == 0:\n",
    "                        axes[assay_env,ee+1].errorbar(meantable['Epoch'],meantable['median_s'],xerr=None,yerr=None,linewidth=1,color='k')\n",
    "                        axes[assay_env][ee+1].tick_params(axis='both',labelsize=7)\n",
    "                \n",
    "                # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "                for ax in axes.flat:\n",
    "                    ax.label_outer()\n",
    "                    ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "                    ax.xaxis.set_ticks(np.arange(0,1200,200))\n",
    "                    \n",
    "                # For home environment plots, thick black outline\n",
    "                thickb = 2\n",
    "                if ploidy == 1 and assay_env == ee:\n",
    "                    axes[assay_env,ee].spines['left'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee].spines['right'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee].spines['top'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee].spines['bottom'].set_linewidth(thickb)\n",
    "                elif ploidy == 0 and assay_env == 2:\n",
    "                    axes[assay_env,ee+1].spines['left'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee+1].spines['right'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee+1].spines['top'].set_linewidth(thickb)\n",
    "                    axes[assay_env,ee+1].spines['bottom'].set_linewidth(thickb)\n",
    "\n",
    "fig.align_ylabels()                \n",
    "\n",
    "\n",
    "#if removingoutliers == True:\n",
    "    #plt.savefig('fig2/fig2_threshold_grey_medianline_v3.jpg',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fig2/fig2_threshold_grey_medianline_v3.pdf',bbox_inches='tight',dpi=3000)\n",
    "#else:\n",
    "    #plt.savefig('fig2/fig2_unfiltered_grey_medianline_v3.jpg',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fig2/fig2_unfiltered_grey_medianline_v3.pdf',bbox_inches='tight',dpi=3000)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 5–figure supplement 2 (Brown-Forsythe test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Browne-Forsythe test to test whether var(home) < var(away)\n",
    "\n",
    "# Formally assess variability across time points and environments using Levene's test (i.e., Browne-Forsythe)\n",
    "# https://en.wikipedia.org/wiki/Levene%27s_test\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html\n",
    "# I'll get the one-sided result as described here:\n",
    "# https://stats.stackexchange.com/questions/69253/one-tailed-levene-test\n",
    "\n",
    "myforlev = finaltable.copy(deep=True)\n",
    "myforlev = myforlev.loc[myforlev['evoEnvt'] != 'YP10%D'].reset_index(drop=True)\n",
    "myforlev['evoEnvt-ploidy'] = myforlev['evoEnvt']+'-'+myforlev['ploidy']\n",
    "\n",
    "eetargets = ['YPD-D','YPD+AA-D','YPD(37C)-D','YPD(37C)-H']\n",
    "eetargetshome = ['30','AA','37','37']\n",
    "\n",
    "levtab = pd.DataFrame()\n",
    "levtab['assay_env'] = envts\n",
    "\n",
    "comparison_counter = 0 # to give the # of comparisons\n",
    "\n",
    "for ee in np.arange(len(eetargets)):\n",
    "    for ae in np.arange(len(envts)):\n",
    "        for g in np.arange(len(epochind)):\n",
    "            if envts[ae] == eetargetshome[ee]:\n",
    "                levtab.at[(levtab['assay_env'] == envts[ae]),eetargets[ee]+'-'+epochind['Epoch'][g]] = np.nan\n",
    "            else:\n",
    "                temp = myforlev.loc[(myforlev['evoEnvt-ploidy'] == eetargets[ee])&(myforlev['Epoch'] == float(epochind['Epoch'][g]))]\n",
    "                # Run brown-forsythe\n",
    "                h = temp['s_'+eetargetshome[ee]+'_adj']\n",
    "                a = temp['s_'+envts[ae]+'_adj']\n",
    "                BF, BFp = stats.levene(h,a,center='median')\n",
    "                \n",
    "                #two-tailed t-test on transformed data\n",
    "                zh = abs(h-statistics.median(h))\n",
    "                za = abs(a-statistics.median(a))\n",
    "                myt, myp_2tail = stats.ttest_ind(zh, za)\n",
    "                \n",
    "                # one-tailed t test p value\n",
    "                # for degrees of freedom, use number of pops minus 1\n",
    "                # subtracting from 1 gives probability home variance is less than away variance\n",
    "                myp_1tail = 1 - stats.t.sf(myt, len(zh)-1)\n",
    "                \n",
    "                levtab.at[(levtab['assay_env'] == envts[ae]),eetargets[ee]+'-'+epochind['Epoch'][g]] = myp_1tail\n",
    "                \n",
    "                comparison_counter = comparison_counter + 1\n",
    "\n",
    "main_alpha = 0.05\n",
    "\n",
    "count_mainalpha_sigp = 0\n",
    "\n",
    "for ee in np.arange(len(eetargets)):\n",
    "    for ae in np.arange(len(envts)):\n",
    "        for g in np.arange(len(epochind)):\n",
    "            thisp = levtab.loc[(levtab['assay_env'] == envts[ae]),eetargets[ee]+'-'+epochind['Epoch'][g]].values[0]\n",
    "            if thisp < main_alpha:\n",
    "                count_mainalpha_sigp = count_mainalpha_sigp + 1\n",
    "\n",
    "count_mainalpha_sigp\n",
    "\n",
    "#export_csv = levtab.to_csv(r'20210420_levene_v01.csv',index=True,header=True)\n",
    "\n",
    "# Make heat map\n",
    "mycolorsforheat = [\"xkcd:baby blue\",'xkcd:grey','xkcd:faded red']\n",
    "\n",
    "mycmap = mcolors.ListedColormap(mycolorsforheat)\n",
    "boundaries = [0,0.05,0.95,1]\n",
    "#boundaries = [0,0.005,0.05,0.10,0.95,1]\n",
    "mynorm = mcolors.BoundaryNorm(boundaries, mycmap.N, clip=True)\n",
    "\n",
    "#levtab = levtab.fillna(1)\n",
    "levtabarr = np.array(levtab[levtab.columns.tolist()[1:]],dtype='f')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,1))\n",
    "\n",
    "ax = sns.heatmap(levtabarr,\n",
    "                 #xticklabels=envts,\n",
    "                 yticklabels=envts,\n",
    "                 #center=0,\n",
    "                 cmap=mycmap,norm=mynorm,cbar=False)\n",
    "\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "ax.set_yticklabels(envts_abbrevofficial,rotation=0)\n",
    "#ax.set_yticklabels(envts,rotation='horizontal')\n",
    "ax.set_xticklabels(epochind['Epoch'].to_list()*6,rotation=45)\n",
    "ax.set_xlabel('Generation')\n",
    "ax.set_ylabel('Assay env.')\n",
    "\n",
    "for ee in np.arange(len(evoenvsno10)):\n",
    "    ax.vlines([6*(ee+1)], *ax.get_ylim())\n",
    "\n",
    "mylw = 4                \n",
    "\n",
    "prange = [str(boundaries[0])+' < p < '+str(boundaries[1]),\n",
    "          str(boundaries[1])+' ≤ p < '+str(boundaries[2]),\n",
    "          str(boundaries[2])+' ≤ p < '+str(boundaries[3])]\n",
    "\n",
    "custom_lines = [Line2D([0], [0], color=mycolorsforheat[0], lw=mylw),\n",
    "                Line2D([0], [0], color=mycolorsforheat[1], lw=mylw),\n",
    "                Line2D([0], [0], color=mycolorsforheat[2], lw=mylw)]\n",
    "\n",
    "plt.rcParams['legend.title_fontsize'] = 7\n",
    "leg = fig.legend(custom_lines,prange,loc='upper left',bbox_to_anchor=(0.91,1.55),\n",
    "                 fontsize=7,title='p range',handlelength=0.3,borderaxespad=1,frameon=False)\n",
    "leg._legend_box.align = \"left\"\n",
    "\n",
    "#if removingoutliers == True:\n",
    "#    plt.savefig('brownforsythe_threshold_v1.jpg',bbox_inches='tight',dpi=300)\n",
    "#    plt.savefig('brownforsythe_threshold_v1.pdf',bbox_inches='tight',dpi=300)\n",
    "#else:\n",
    "#    plt.savefig('brownforsythe_unfiltered_v1.jpg',bbox_inches='tight',dpi=300)\n",
    "#    plt.savefig('brownforsythe_unfiltered_v1.pdf',bbox_inches='tight',dpi=300)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize movement in 2D envt spaces\n",
    "\n",
    "wtab = finaltable.copy(deep=True)\n",
    "\n",
    "# start with an evocode_list that is without YP10\n",
    "evocode_listupdate = new_evocode_list.copy(deep = True)\n",
    "evocode_listupdate['evoEnvt-ploidy'] = evocode_listupdate['evoEnvt']+\"-\"+evocode_listupdate['ploidy'].map(str)\n",
    "evocode_listupdate['evoEnvt-ploidy-LP'] = evocode_listupdate['evoEnvt-ploidy']+\"-\"+evocode_listupdate['LP'].map(str)\n",
    "# remove the YP10 guys\n",
    "tentest = evocode_listupdate['evoEnvt'] != 'YP10%D'\n",
    "evocode_listupdate = evocode_listupdate[tentest]\n",
    "evocode_listupdate = evocode_listupdate.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# create comprehensive list of assay environment pairs\n",
    "envtpairsno10 = [['30','37'],['30','AA'],['30','RT'],['30','NaCl'],['37','AA'],['37','RT'],['37','NaCl'],['AA','RT'],['AA','NaCl'],['RT','NaCl']]\n",
    "\n",
    "mywidth = 0.5\n",
    "myalpha = 0.5\n",
    "myalphas = [0.3,0.4,0.5,0.6,0.7]\n",
    "mywidths = [1.5,1.5,1.5,1.5,1.5]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(9,9), sharex=False, sharey=False)\n",
    "\n",
    "names = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "\n",
    "for ax, col in zip(axes[0], names):\n",
    "    ax.set_title(col, size=8, fontweight='bold')\n",
    "\n",
    "for ax, row in zip(axes[:,0], names):\n",
    "    ax.set_ylabel(row, rotation=90, size=8, fontweight='bold')\n",
    "    \n",
    "for e1 in np.arange(len(envts)):\n",
    "    for e2 in np.arange(len(envts)):\n",
    "        for epoch in np.arange(1):\n",
    "            subt = wtab.loc[(wtab['Epoch'] <= float(epochind['Epoch'][epoch+5]))]\n",
    "            for ec in np.arange(len(evocode_listupdate)):\n",
    "                subtec = subt.loc[(subt['evocode'] == evocode_listupdate.loc[ec,'evocode'])]\n",
    "                if evocode_listupdate.loc[ec,'ploidy'] == 'H':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#D81B60',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD(37C)':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#E1AB06',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#2497FD',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD+AA':    \n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#025F17',alpha=myalpha,linewidth=mywidth)\n",
    "                axes[e2,e1].tick_params(axis='both',labelsize=7)\n",
    "            \n",
    "            # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "            for ax in axes.flat:\n",
    "                ax.label_outer()\n",
    "                ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "                ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "        axes[e2,e1].axhline(y=0,linewidth=0.5,color='xkcd:grey')\n",
    "        axes[e2,e1].axvline(x=0,linewidth=0.5,color='xkcd:grey')\n",
    "                \n",
    "mylw = 2                \n",
    "eenames = ['YPD','YPD + Acetic acid','YPD, 37°C (diploid)','YPD, 37°C (haploid)']\n",
    "custom_lines = [Line2D([0], [0], color='#2497FD', lw=mylw),\n",
    "                Line2D([0], [0], color='#025F17', lw=mylw),\n",
    "                Line2D([0], [0], color='#E1AB06', lw=mylw),\n",
    "                Line2D([0], [0], color='#D81B60', lw=mylw)]\n",
    "\n",
    "plt.rcParams['legend.title_fontsize'] = 7\n",
    "leg = fig.legend(custom_lines,eenames,loc='upper left',bbox_to_anchor=(0.26, 0.15, 0.5, 0.5),\n",
    "                 fontsize=7,title='Evolution environment')\n",
    "leg._legend_box.align = \"left\"\n",
    "\n",
    "fig.align_ylabels()\n",
    "\n",
    "\n",
    "# remove redundant plots\n",
    "delax = [[0,0],[0,1],[0,2],[0,3],[0,4]\n",
    "        ,[1,1],[1,2],[1,3],[1,4]\n",
    "        ,[2,2],[2,3],[2,4]\n",
    "        ,[3,3],[3,4]\n",
    "        ,[4,4]]\n",
    "for pair in np.arange(len(delax)):\n",
    "    fig.delaxes(axes[delax[pair][0]][delax[pair][1]])\n",
    "    \n",
    "for i in np.arange(4):\n",
    "    axes[4,i].set_xlabel(names[i],fontsize=8,fontweight='bold')\n",
    "\n",
    "    \n",
    "fig.text(0.43, 0.065, 'Fitness in assay environment', ha='center',size=10, fontweight = 'bold')\n",
    "fig.text(0.04, 0.43, 'Fitness in assay environment', va='center', rotation='vertical',size=10, fontweight = 'bold')\n",
    "\n",
    "# For the gif below, get the axes limits\n",
    "xlimlist = []\n",
    "ylimlist = []\n",
    "for i in np.arange(4):\n",
    "    xlimlist = xlimlist + [axes[4][i].get_xlim()]\n",
    "    ylimlist = ylimlist + [axes[i+1][0].get_ylim()]\n",
    "\n",
    "#if removingoutliers == True:\n",
    "    #plt.savefig('fig3/fig3_threshold_v12.jpg',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fig3/fig3_threshold_v12.pdf',bbox_inches='tight',dpi=3000)\n",
    "#else:\n",
    "    #plt.savefig('fig3/fig3_unfiltered_v12.jpg',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fig3/fig3_unfiltered_v12.pdf',bbox_inches='tight',dpi=3000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 .gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 6 frames that will make up a gif\n",
    "# Run only after the above, which gets the axes limits\n",
    "\n",
    "wtab = finaltable.copy(deep=True)\n",
    "\n",
    "# start with an evocode_list that is without YP10\n",
    "evocode_listupdate = new_evocode_list.copy(deep = True)\n",
    "evocode_listupdate['evoEnvt-ploidy'] = evocode_listupdate['evoEnvt']+\"-\"+evocode_listupdate['ploidy'].map(str)\n",
    "evocode_listupdate['evoEnvt-ploidy-LP'] = evocode_listupdate['evoEnvt-ploidy']+\"-\"+evocode_listupdate['LP'].map(str)\n",
    "# remove the YP10 guys\n",
    "tentest = evocode_listupdate['evoEnvt'] != 'YP10%D'\n",
    "evocode_listupdate = evocode_listupdate[tentest]\n",
    "evocode_listupdate = evocode_listupdate.reset_index(drop=True)\n",
    "\n",
    "# create comprehensive list of assay environment pairs\n",
    "envtpairsno10 = [['30','37'],['30','AA'],['30','RT'],['30','NaCl'],['37','AA'],['37','RT'],['37','NaCl'],['AA','RT'],['AA','NaCl'],['RT','NaCl']]\n",
    "\n",
    "mywidth = 0.5\n",
    "myalpha = 0.5\n",
    "myalphas = [0.3,0.4,0.5,0.6,0.7]\n",
    "mywidths = [1.5,1.5,1.5,1.5,1.5]\n",
    "\n",
    "names = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "\n",
    "filenames_gif_thresh = []\n",
    "filenames_gif_unfil = []\n",
    "\n",
    "for g in np.arange(1,len(epochind)):\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(9,9), sharex=False, sharey=False)\n",
    "\n",
    "    for ax, col in zip(axes[0], names):\n",
    "        ax.set_title(col, size=8, fontweight='bold')\n",
    "\n",
    "    for ax, row in zip(axes[:,0], names):\n",
    "        ax.set_ylabel(row, rotation=90, size=8, fontweight='bold')\n",
    "\n",
    "    for e1 in np.arange(len(envts)):\n",
    "        for e2 in np.arange(len(envts)):\n",
    "            subt = wtab.loc[(wtab['Epoch'] <= float(epochind['Epoch'][g]))]\n",
    "            for ec in np.arange(len(evocode_listupdate)):\n",
    "                subtec = subt.loc[(subt['evocode'] == evocode_listupdate.loc[ec,'evocode'])]\n",
    "                if evocode_listupdate.loc[ec,'ploidy'] == 'H':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#D81B60',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD(37C)':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#E1AB06',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD':\n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#2497FD',alpha=myalpha,linewidth=mywidth)\n",
    "                elif evocode_listupdate.loc[ec,'evoEnvt'] == 'YPD+AA':    \n",
    "                    axes[e2,e1].plot(subtec['s_'+envts[e1]+'_adj'],subtec['s_'+envts[e2]+'_adj'],color='#025F17',alpha=myalpha,linewidth=mywidth)\n",
    "                axes[e2,e1].tick_params(axis='both',labelsize=7)\n",
    "            \n",
    "            # set x and y limits\n",
    "            if e1 < 4:\n",
    "                axes[e2,e1].set_xlim(xlimlist[e1])\n",
    "            if e2 > 0:\n",
    "                axes[e2,e1].set_ylim(ylimlist[e2-1])\n",
    "\n",
    "            # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "            for ax in axes.flat:\n",
    "                ax.label_outer()\n",
    "                ax.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "                ax.xaxis.set_major_formatter(mtick.FormatStrFormatter('%.2f'))\n",
    "            axes[e2,e1].axhline(y=0,linewidth=0.5,color='xkcd:grey')\n",
    "            axes[e2,e1].axvline(x=0,linewidth=0.5,color='xkcd:grey')\n",
    "\n",
    "    mylw = 2                \n",
    "    eenames = ['YPD','YPD + Acetic acid','YPD, 37°C (diploid)','YPD, 37°C (haploid)']\n",
    "    custom_lines = [Line2D([0], [0], color='#2497FD', lw=mylw),\n",
    "                    Line2D([0], [0], color='#025F17', lw=mylw),\n",
    "                    Line2D([0], [0], color='#E1AB06', lw=mylw),\n",
    "                    Line2D([0], [0], color='#D81B60', lw=mylw)]\n",
    "\n",
    "    plt.rcParams['legend.title_fontsize'] = 7\n",
    "    leg = fig.legend(custom_lines,eenames,loc='upper left',bbox_to_anchor=(0.26, 0.15, 0.5, 0.5),\n",
    "                     fontsize=7,title='Evolution environment')\n",
    "    leg._legend_box.align = \"left\"\n",
    "\n",
    "    fig.align_ylabels()\n",
    "\n",
    "\n",
    "    # remove redundant plots\n",
    "    delax = [[0,0],[0,1],[0,2],[0,3],[0,4]\n",
    "            ,[1,1],[1,2],[1,3],[1,4]\n",
    "            ,[2,2],[2,3],[2,4]\n",
    "            ,[3,3],[3,4]\n",
    "            ,[4,4]]\n",
    "    for pair in np.arange(len(delax)):\n",
    "        fig.delaxes(axes[delax[pair][0]][delax[pair][1]])\n",
    "\n",
    "    for i in np.arange(4):\n",
    "        axes[4,i].set_xlabel(names[i],fontsize=8,fontweight='bold')\n",
    "\n",
    "    fig.text(0.43, 0.065, 'Fitness in assay environment', ha='center',size=10, fontweight = 'bold')\n",
    "    fig.text(0.04, 0.43, 'Fitness in assay environment', va='center', rotation='vertical',size=10, fontweight = 'bold')\n",
    "    \n",
    "    # Add a generation counter thing\n",
    "    fig.text(0.51,0.5,'Generation '+epochind['Epoch'][g],size=18)\n",
    "\n",
    "    if removingoutliers == True:\n",
    "        f = 'fig3/fig3_threshold_'+epochind['Epoch'][g]+'.jpg'\n",
    "        plt.savefig(f,bbox_inches='tight',dpi=300)\n",
    "        filenames_gif_thresh = filenames_gif_thresh + [f]\n",
    "    else:\n",
    "        f = 'fig3/fig3_unfiltered_'+epochind['Epoch'][g]+'.jpg'\n",
    "        plt.savefig(f,bbox_inches='tight',dpi=300)\n",
    "        filenames_gif_unfil = filenames_gif_unfil + [f]\n",
    "\n",
    "# Generate the gif\n",
    "myd = 1\n",
    "\n",
    "if removingoutliers == True:    \n",
    "    images = []\n",
    "    for filename in filenames_gif_thresh:\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave('Fig3_thresh.gif', images, duration=myd)\n",
    "else:\n",
    "    images = []\n",
    "    for filename in filenames_gif_unfil:\n",
    "        images.append(imageio.imread(filename))\n",
    "    imageio.mimsave('Fig3_unfil.gif', images, duration=myd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate s bootstraps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final s table\n",
    "if removingoutliers == True:\n",
    "    myfinaltable = pd.read_csv(\"20210413_final_s_table_threshold.csv\")\n",
    "else:\n",
    "    myfinaltable = pd.read_csv(\"20210413_final_s_table_unfiltered.csv\")\n",
    "\n",
    "# Generate bootstrap tables\n",
    "\n",
    "nboot = 100\n",
    "\n",
    "intervals = ['10-30','30-50']\n",
    "\n",
    "#for n in np.arange(1):\n",
    "for n in np.arange(nboot):\n",
    "    boots = pd.DataFrame()\n",
    "    boots['Epoch'] = myfinaltable['Epoch']\n",
    "    boots['LP'] = myfinaltable['LP']\n",
    "    boots['evoEnvt'] = myfinaltable['evoEnvt']\n",
    "    boots['evocode'] = myfinaltable['evocode']\n",
    "    boots['evocode-Epoch'] = myfinaltable['evocode-Epoch']\n",
    "    boots['ploidy'] = myfinaltable['ploidy']\n",
    "    boots['Epoch-true'] = myfinaltable['Epoch-true']\n",
    "    \n",
    "    for envt in np.arange(len(envts)):\n",
    "        new_s = []\n",
    "        for evep in np.arange(len(boots)):\n",
    "            # identify the possible interval s values to choose from\n",
    "            shoebox = []\n",
    "            for i in np.arange(len(intervals)):\n",
    "                for rep in np.arange(2):\n",
    "                    if ~np.isnan(myfinaltable.loc[evep,'s_'+intervals[i]+'_'+envts[envt]+'-rep'+reps[rep]]):\n",
    "                        shoebox = shoebox + [myfinaltable.loc[evep,'s_'+intervals[i]+'_'+envts[envt]+'-rep'+reps[rep]]]\n",
    "            # choose interval s values for the bootstrap\n",
    "            newshoes = []\n",
    "            for sb in np.arange(len(shoebox)):\n",
    "                newshoes = newshoes + [shoebox[randrange(len(shoebox))]]\n",
    "            new_s = new_s + [np.nanmean(newshoes)]\n",
    "        boots['s_'+envts[envt]] = new_s\n",
    "        \n",
    "    # Now create the s_adj values that interpolate/extrapolate fitness for the p2 populations\n",
    "    for envt in np.arange(len(envts)):\n",
    "        boots['s_'+envts[envt]+'_adj'] = \"\"\n",
    "    \n",
    "    for envt in np.arange(len(envts)):\n",
    "        for ece in np.arange(len(boots)):\n",
    "            if boots.loc[ece,'evocode'][1] == '2' and boots.loc[ece,'Epoch'] > 0:\n",
    "                if boots.loc[ece,'Epoch-true'] < 960:\n",
    "                    this_s = boots.loc[ece,'s_'+envts[envt]]\n",
    "                    next_s = boots.loc[(boots['evocode'] == boots.loc[ece,'evocode']) &\n",
    "                                            (boots['Epoch-true'] == boots.loc[ece,'Epoch-true'] + 200)]['s_'+envts[envt]].iloc[0]\n",
    "                    my_m = (next_s - this_s)/200\n",
    "                    my_b = this_s - boots.loc[ece,'Epoch-true'] * my_m\n",
    "                    new_s = my_m * boots.loc[ece,'Epoch'] + my_b\n",
    "                    boots.at[ece,'s_'+envts[envt]+'_adj'] = new_s\n",
    "                else:\n",
    "                    this_s = boots.loc[ece,'s_'+envts[envt]]\n",
    "                    prev_s = boots.loc[(boots['evocode'] == boots.loc[ece,'evocode']) &\n",
    "                                            (boots['Epoch-true'] == boots.loc[ece,'Epoch-true'] - 200)]['s_'+envts[envt]].iloc[0]\n",
    "                    my_m = (this_s - prev_s)/200\n",
    "                    my_b = this_s - boots.loc[ece,'Epoch-true'] * my_m\n",
    "                    new_s = my_m * boots.loc[ece,'Epoch'] + my_b\n",
    "                    boots.at[ece,'s_'+envts[envt]+'_adj'] = new_s                \n",
    "            else:\n",
    "                boots.at[ece,'s_'+envts[envt]+'_adj'] = boots.loc[ece,'s_'+envts[envt]] \n",
    "    \n",
    "    # GENERATE BOOTSTRAPS HERE\n",
    "    #if removingoutliers == True:\n",
    "        #export_csv = boots.to_csv(r'bootstraps/s/20210413_final_s_table_threshold_'+str(n)+'_wadj.csv',index=True,header=True)\n",
    "    #else:\n",
    "        #export_csv = boots.to_csv(r'bootstraps/s/20210413_final_s_table_unfiltered_'+str(n)+'_wadj.csv',index=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 4 (PCAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Figure 4A\n",
    "\n",
    "# Redo what's immediately above in 2r3c format\n",
    "\n",
    "fig0,ax0 = plt.subplots(nrows=2, ncols=3, figsize=(5,3*0.83),sharey='row',sharex=False)\n",
    "fig0.subplots_adjust(hspace=0.3,wspace=0.2)\n",
    "\n",
    "# DO THE 6 TIMECOURSE SUBPLOTS\n",
    "# start with an evocode_list that is without YP10\n",
    "evocode_listupdate = new_evocode_list.copy(deep = True)\n",
    "evocode_listupdate['evoEnvt-ploidy'] = evocode_listupdate['evoEnvt']+\"-\"+evocode_listupdate['ploidy'].map(str)\n",
    "evocode_listupdate['evoEnvt-ploidy-LP'] = evocode_listupdate['evoEnvt-ploidy']+\"-\"+evocode_listupdate['LP'].map(str)\n",
    "# remove the YP10 guys\n",
    "tentest = evocode_listupdate['evoEnvt'] != 'YP10%D'\n",
    "evocode_listupdate = evocode_listupdate[tentest]\n",
    "evocode_listupdate = evocode_listupdate.reset_index(drop=True)\n",
    "\n",
    "# go through the evocode list and create a column for each fitness at each epoch (AA, NaCl, etc.). Basically,\n",
    "# just locate the evocode-epoch (e.g., p8-D3) and fill in the appropriate fitness values into each of 5 lists.\n",
    "# this first one will be without breaking out the RT into 10-30 and 30-50\n",
    "\n",
    "endtable = evocode_listupdate\n",
    "\n",
    "for e in np.arange(len(epochind)):\n",
    "    YPDendlist = []\n",
    "    YPD37endlist = []\n",
    "    AAendlist = []\n",
    "    RTendlist = []\n",
    "    NaClendlist = []\n",
    "\n",
    "    for ec in np.arange(len(evocode_listupdate)):\n",
    "        picked = finaltable.loc[finaltable['evocode-Epoch'] == evocode_listupdate['evocode'][ec]+'-'+epochind['Epoch'][e]]\n",
    "        picked = picked.reset_index(drop=True)\n",
    "        YPDendlist = YPDendlist + [picked.loc[0,'s_30_adj']]\n",
    "        YPD37endlist = YPD37endlist + [picked.loc[0,'s_37_adj']]\n",
    "        AAendlist = AAendlist + [picked.loc[0,'s_AA_adj']]\n",
    "        RTendlist = RTendlist + [picked.loc[0,'s_RT_adj']]\n",
    "        NaClendlist = NaClendlist + [picked.loc[0,'s_NaCl_adj']]\n",
    "\n",
    "    endtable['30-'+epochind['Epoch'][e]] = YPDendlist\n",
    "    endtable['37-'+epochind['Epoch'][e]] = YPD37endlist\n",
    "    endtable['AA-'+epochind['Epoch'][e]] = AAendlist\n",
    "    endtable['RT-'+epochind['Epoch'][e]] = RTendlist\n",
    "    endtable['NaCl-'+epochind['Epoch'][e]] = NaClendlist\n",
    "\n",
    "\n",
    "# now normalize this endtable data (standardization), since PCA output influenced by scale of data features.\n",
    "# ref: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
    "# Apply normalization with sklearn's StandardScaler. Want it to normalize such that it has a mean of 0, sd of 1,\n",
    "# Normally distributed.\n",
    "\n",
    "# more useful definitions for the unfurled loop :(\n",
    "npc_end = 2\n",
    "targets = ['YPD-D','YPD+AA-D','YPD(37C)-D','YPD(37C)-H']\n",
    "newnames = ['YPD','YPD + Acetic acid','YPD, 37°C (diploid)','YPD, 37°C (haploid)']\n",
    "colors = ['#2497FD', '#025F17', '#E1AB06','#D81B60']\n",
    "\n",
    "if removingoutliers == True:\n",
    "    ylimlow = -4.36\n",
    "    ylimhigh = 6.54\n",
    "else:\n",
    "    ylimlow = -5\n",
    "    ylimhigh = 6.5\n",
    "\n",
    "# Note that the below plotting will cause deprecation warnings to come up, but these can be safely ignored\n",
    "# according to the internet.\n",
    "\n",
    "axlist = [[0,0],[0,1],[0,2],\n",
    "          [1,0],[1,1],[1,2]]\n",
    "\n",
    "for e in np.arange(6):\n",
    "    specendtable = endtable[list(endtable)[:6]+list(endtable)[6+5*e:11+5*e]].copy(deep=True)\n",
    "\n",
    "    endfeatures = list(specendtable)[6:len(list(specendtable))]\n",
    "    norm_endtable = specendtable.loc[:,endfeatures].values\n",
    "    norm_endtable = StandardScaler().fit_transform(norm_endtable)\n",
    "\n",
    "    # now re-project data to n principle components\n",
    "    pca_end = PCA(n_components = npc_end)\n",
    "    principalcomponents_end = pca_end.fit_transform(norm_endtable)\n",
    "\n",
    "    # plot now\n",
    "    pcend_df = pd.DataFrame(data = principalcomponents_end\n",
    "                           , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = specendtable['evoEnvt-ploidy'] == target\n",
    "        if removingoutliers == True:\n",
    "            if e < 5:\n",
    "                ax0[axlist[e][0]][axlist[e][1]].scatter(pcend_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                            pcend_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 6, alpha=0.7)\n",
    "            else:\n",
    "                ax0[axlist[e][0]][axlist[e][1]].scatter(-pcend_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                            pcend_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 6, alpha=0.7)\n",
    "        if removingoutliers == False:\n",
    "            if e != 1:\n",
    "                ax0[axlist[e][0]][axlist[e][1]].scatter(pcend_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                            pcend_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 6, alpha=0.7)\n",
    "            else:\n",
    "                ax0[axlist[e][0]][axlist[e][1]].scatter(pcend_df.loc[indicesToKeep, 'principal component 1'],\n",
    "                            -pcend_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 6, alpha=0.7)\n",
    "        ax0[axlist[e][0]][axlist[e][1]].set_ylim([ylimlow,ylimhigh])\n",
    "    \n",
    "    ax0[axlist[e][0]][axlist[e][1]].tick_params(axis='both',labelsize=7)\n",
    "\n",
    "    if e == 0:\n",
    "        ax0[axlist[e][0]][axlist[e][1]].set_xticks([-5,0,5])\n",
    "    else:\n",
    "        ax0[axlist[e][0]][axlist[e][1]].set_xticks([-2,0,2])\n",
    "    \n",
    "    # Put on generation labels\n",
    "    ydiff = ylimhigh - ylimlow\n",
    "    diffx = ax0[axlist[e][0]][axlist[e][1]].get_xlim()[1]- ax0[axlist[e][0]][axlist[e][1]].get_xlim()[0]\n",
    "    ax0[axlist[e][0]][axlist[e][1]].text(ax0[axlist[e][0]][axlist[e][1]].get_xlim()[1] - diffx*0.05,ylimhigh - ydiff*0.05,epochind['Epoch'][e],\n",
    "               verticalalignment='top',horizontalalignment='right',fontsize=8)\n",
    "\n",
    "#ax0[1][2].set_xlabel('Principal component - 1',fontsize=8)\n",
    "#fig0.text(0.5, 0.08, 'Principal component - 1', ha='center',size=8)\n",
    "fig0.text(0.06, 0.5, 'Principal component - 2', va='center', rotation='vertical',size=8)\n",
    "fig0.text(0.4, 0.0, 'Principal component - 1', va='center', rotation='horizontal',size=8)\n",
    "    \n",
    "# Add legend\n",
    "leg = fig0.legend(newnames,fontsize=7,loc='upper left',handletextpad=0, borderaxespad=1, ncol=4,columnspacing=0.5,\n",
    "                 title='Evolution environment',bbox_to_anchor=(0.03, 1.15))\n",
    "\n",
    "leg._legend_box.align = \"left\"\n",
    "\n",
    "# Legend not appearing great here, so export separately per this:\n",
    "# https://stackoverflow.com/questions/4534480/get-legend-as-a-separate-picture-in-matplotlib\n",
    "def export_legend(legend, filename=\"legend.png\", expand=[-5,-5,5,5]):\n",
    "    fig  = legend.figure\n",
    "    fig.canvas.draw()\n",
    "    bbox  = legend.get_window_extent()\n",
    "    bbox = bbox.from_extents(*(bbox.extents + np.array(expand)))\n",
    "    bbox = bbox.transformed(fig.dpi_scale_trans.inverted())\n",
    "    fig.savefig(filename, dpi=\"figure\", bbox_inches=bbox)\n",
    "\n",
    "#export_legend(leg,filename='fig6/PCA/evoenvleg.pdf')\n",
    "#export_legend(leg,filename='fig6/PCA/evoenvleg.jpg')\n",
    "\n",
    "#if removingoutliers == True:\n",
    "    #plt.savefig('fig6/PCA/4A_3col_thres.pdf',bbox_inches='tight',dpi=300)\n",
    "    #plt.savefig('fig6/PCA/4A_3col_thres.jpg',bbox_inches='tight',dpi=300)\n",
    "#else:\n",
    "    #plt.savefig('fig6/PCA/4A_3col_unfil.pdf',bbox_inches='tight',dpi=300)\n",
    "    #plt.savefig('fig6/PCA/4A_3col_unfil.jpg',bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4B\n",
    "# Repackage the all-data PCA for standalone export\n",
    "\n",
    "# start with an evocode_list that is without YP10\n",
    "evocode_listupdate = new_evocode_list.copy(deep = True)\n",
    "evocode_listupdate['evoEnvt-ploidy'] = evocode_listupdate['evoEnvt']+\"-\"+evocode_listupdate['ploidy'].map(str)\n",
    "evocode_listupdate['evoEnvt-ploidy-LP'] = evocode_listupdate['evoEnvt-ploidy']+\"-\"+evocode_listupdate['LP'].map(str)\n",
    "# remove the YP10 guys\n",
    "tentest = evocode_listupdate['evoEnvt'] != 'YP10%D'\n",
    "evocode_listupdate = evocode_listupdate[tentest]\n",
    "evocode_listupdate = evocode_listupdate.reset_index(drop=True)\n",
    "\n",
    "# go through the evocode list and create a column for each fitness at each epoch (AA, NaCl, etc.). Basically,\n",
    "# just locate the evocode-epoch (e.g., p8-D3) and fill in the appropriate fitness values into each of 5 lists.\n",
    "# this first one will be without breaking out the RT into 10-30 and 30-50\n",
    "\n",
    "endtable = evocode_listupdate\n",
    "\n",
    "for e in np.arange(len(epochind)):\n",
    "    YPDendlist = []\n",
    "    YPD37endlist = []\n",
    "    AAendlist = []\n",
    "    RTendlist = []\n",
    "    NaClendlist = []\n",
    "\n",
    "    for ec in np.arange(len(evocode_listupdate)):\n",
    "        picked = finaltable.loc[finaltable['evocode-Epoch'] == evocode_listupdate['evocode'][ec]+'-'+epochind['Epoch'][e]]\n",
    "        picked = picked.reset_index(drop=True)\n",
    "        YPDendlist = YPDendlist + [picked.loc[0,'s_30_adj']]\n",
    "        YPD37endlist = YPD37endlist + [picked.loc[0,'s_37_adj']]\n",
    "        AAendlist = AAendlist + [picked.loc[0,'s_AA_adj']]\n",
    "        RTendlist = RTendlist + [picked.loc[0,'s_RT_adj']]\n",
    "        NaClendlist = NaClendlist + [picked.loc[0,'s_NaCl_adj']]\n",
    "\n",
    "    endtable['30-'+epochind['Epoch'][e]] = YPDendlist\n",
    "    endtable['37-'+epochind['Epoch'][e]] = YPD37endlist\n",
    "    endtable['AA-'+epochind['Epoch'][e]] = AAendlist\n",
    "    endtable['RT-'+epochind['Epoch'][e]] = RTendlist\n",
    "    endtable['NaCl-'+epochind['Epoch'][e]] = NaClendlist\n",
    "    \n",
    "    \n",
    "# now normalize this endtable data (standardization), since PCA output influenced by scale of data features.\n",
    "# ref: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
    "# Apply normalization with sklearn's StandardScaler. Want it to normalize such that it has a mean of 0, sd of 1,\n",
    "# Normally distributed.\n",
    "\n",
    "endfeatures = list(endtable)[6:len(list(endtable))]\n",
    "norm_endtable = endtable.loc[:,endfeatures].values\n",
    "norm_endtable = StandardScaler().fit_transform(norm_endtable)\n",
    "\n",
    "# now re-project data to n principle components\n",
    "npc_end = 2\n",
    "pca_end = PCA(n_components = npc_end)\n",
    "principalcomponents_end = pca_end.fit_transform(norm_endtable)\n",
    "\n",
    "# look at the variance explained\n",
    "#print('Explained variation per principal component: {}'.format(pca_end.explained_variance_ratio_))\n",
    "\n",
    "# plot\n",
    "pcend_df = pd.DataFrame(data = principalcomponents_end\n",
    "                       , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "figm = plt.figure(figsize=(1.5/(pca_end.explained_variance_ratio_[1]/pca_end.explained_variance_ratio_[0]),1.5))\n",
    "axm = figm.add_subplot(1, 1, 1)\n",
    "\n",
    "targets = ['YPD-D','YPD+AA-D','YPD(37C)-D','YPD(37C)-H']\n",
    "newnames = ['YPD','YPD + Acetic acid','YPD, 37°C, Diploid','YPD, 37°C, Haploid']\n",
    "\n",
    "colors = ['#2497FD', '#025F17', '#E1AB06','#D81B60']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = endtable['evoEnvt-ploidy'] == target\n",
    "    axm.scatter(pcend_df.loc[indicesToKeep, 'principal component 1']\n",
    "               , pcend_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 12,alpha=0.7)\n",
    "    \n",
    "axm.tick_params(axis='both',labelsize=7)\n",
    "axm.yaxis.set_ticks(np.arange(-5,11,5))\n",
    "\n",
    "axm.set_xlabel('Principal Component - 1',size=8)\n",
    "axm.set_ylabel('Principal Component - 2',rotation=90,size=8)\n",
    "\n",
    "#if removingoutliers == True:\n",
    "    #plt.savefig('fig6/PCA/4B_threshold_v01.pdf',bbox_inches='tight',dpi=300)\n",
    "    #plt.savefig('fig6/PCA/4B_threshold_v01.jpg',bbox_inches='tight',dpi=300)\n",
    "#else:\n",
    "    #plt.savefig('fig6/PCA/4B_unfiltered_v01.pdf',bbox_inches='tight',dpi=300)\n",
    "    #plt.savefig('fig6/PCA/4B_unfiltered_v01.jpg',bbox_inches='tight',dpi=300)\n",
    "\n",
    "# FOR FUTURE REFERENCE (SEE CALLOUTS BELOW), MERGE POP INFO WITH PCEND_DF\n",
    "fullpcmap = endtable.copy(deep=True)\n",
    "fullpcmap['principal component 1'] = pcend_df['principal component 1']\n",
    "fullpcmap['principal component 2'] = pcend_df['principal component 2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4C\n",
    "# Callout populations\n",
    "\n",
    "# create a list of values specifically for one evocode at a time\n",
    "\n",
    "targets = ['30','AA','37','RT','NaCl']\n",
    "newnames = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "colors = ['#2497FD', '#025F17', '#E1AB06','#6B2E58','#A48DE2']\n",
    "\n",
    "callout_list = ['p6-H7','p1-D5','p3-D8','p8-G3',\n",
    "                'p6-C8','p4-E6','p7-A7','p8-C8']\n",
    "\n",
    "fig,ax = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True,figsize=(3*1.7,2.2))\n",
    "#fig.subplots_adust()\n",
    "\n",
    "coords = [[0,0],[0,1],[0,2],[0,3],[1,0],[1,1],[1,2],[1,3]]\n",
    "\n",
    "for i in np.arange(len(callout_list)):\n",
    "    temp = finaltable.loc[(finaltable['evocode'] == callout_list[i])].copy(deep=True).sort_values(by='Epoch').reset_index(drop=True)\n",
    "    homeenv = envts[evoenvs.index(temp.loc[0,'evoEnvt'])]\n",
    "    homeevo = temp.loc[0,'evoEnvt']\n",
    "    for e in np.arange(len(envts)):\n",
    "        if envts[e] == homeenv:\n",
    "            ax[coords[i][0]][coords[i][1]].errorbar(temp['Epoch'],temp['s_'+envts[e]+'_adj'],xerr=None,yerr=temp['stderr(s)_'+envts[e]],c=colors[e])\n",
    "            if temp.loc[0,'ploidy'] == 'H':\n",
    "                plt.setp(ax[coords[i][0]][coords[i][1]].spines.values(),color='#D81B60',lw=1.25,alpha=0.7)\n",
    "                plt.setp([ax[coords[i][0]][coords[i][1]].get_xticklines(),ax[coords[i][0]][coords[i][1]].get_yticklines()],color='#D81B60')\n",
    "            else:\n",
    "                plt.setp(ax[coords[i][0]][coords[i][1]].spines.values(),color=colors[e],lw=1.25,alpha=0.7)\n",
    "                plt.setp([ax[coords[i][0]][coords[i][1]].get_xticklines(),ax[coords[i][0]][coords[i][1]].get_yticklines()],color=colors[e])\n",
    "        else:\n",
    "            ax[coords[i][0]][coords[i][1]].errorbar(temp['Epoch'],temp['s_'+envts[e]+'_adj'],xerr=None,yerr=temp['stderr(s)_'+envts[e]],c=colors[e],linestyle='--')        \n",
    "\n",
    "    ax[coords[i][0]][coords[i][1]].tick_params(axis='both',labelsize=7)\n",
    "    ax[coords[i][0]][coords[i][1]].set_yticks([-0.2,-0.1,0,0.1,0.2])\n",
    "    ax[coords[i][0]][coords[i][1]].set_xticks([0,200,400,600,800,1000])\n",
    "    ax[coords[i][0]][coords[i][1]].set_xticklabels([0,200,400,600,800,1000],Rotation=45)\n",
    "    ax[coords[i][0]][coords[i][1]].axhline(y=0,lw=0.5,color='xkcd:grey',zorder=0)\n",
    "    \n",
    "    \n",
    "fig.text(0.52, -0.08, 'Generation', ha='center',size=8)\n",
    "fig.text(0.04, 0.5, 'Fitness', va='center', rotation='vertical',size=8)\n",
    "\n",
    "#plt.savefig('fig6/callouts_v01.pdf',bbox_inches='tight',dpi=300)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4D\n",
    "# Note, the PCA bootstrap generation is not included here\n",
    "\n",
    "if removingoutliers == True:\n",
    "    filtering = \"thres\"\n",
    "else:\n",
    "    filtering = \"unfil\"\n",
    "\n",
    "fig = plt.figure(figsize=(1.5/(pca_end.explained_variance_ratio_[1]/pca_end.explained_variance_ratio_[0]),1.5))\n",
    "axclus = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "#importing data for plotting\n",
    "true_means = pd.DataFrame(pd.read_csv('for_CWB/%s_true_means.csv' % filtering,delimiter=','))\n",
    "true_cum = pd.DataFrame(pd.read_csv('for_CWB/%s_true_cum.csv'% filtering,delimiter=','))\n",
    "bs_low_all = pd.DataFrame(pd.read_csv('for_CWB/%s_bs_low_all.csv'% filtering,delimiter=','))\n",
    "bs_high_all = pd.DataFrame(pd.read_csv('for_CWB/%s_bs_high_all.csv'% filtering,delimiter=','))\n",
    "bs_low_cum = pd.DataFrame(pd.read_csv('for_CWB/%s_bs_low_cum.csv'% filtering,delimiter=','))\n",
    "bs_high_cum = pd.DataFrame(pd.read_csv('for_CWB/%s_bs_high_cum.csv'% filtering,delimiter=','))\n",
    "\n",
    "########data\n",
    "#YPD_s30 data\n",
    "true_YPD_mean = np.array(true_means.YPD)\n",
    "bs_YPD_lo = np.array(bs_low_all.YPD)\n",
    "bs_YPD_hi = np.array(bs_high_all.YPD)\n",
    "bs_YPD_CI = [bs_YPD_lo,bs_YPD_hi]\n",
    "#YPD_s37 data\n",
    "true_YPD37_mean = np.array(true_means.YPD37)\n",
    "bs_YPD37_lo = np.array(bs_low_all.YPD37)\n",
    "bs_YPD37_hi = np.array(bs_high_all.YPD37)\n",
    "bs_YPD37_CI = [bs_YPD37_lo,bs_YPD37_hi]\n",
    "#YPD_s37H data\n",
    "true_YPD37H_mean = np.array(true_means.YPD37H)\n",
    "bs_YPD37H_lo = np.array(bs_low_all.YPD37H)\n",
    "bs_YPD37H_hi = np.array(bs_high_all.YPD37H)\n",
    "bs_YPD37H_CI = [bs_YPD37H_lo,bs_YPD37H_hi]\n",
    "#YPD_sAA data\n",
    "true_YPDAA_mean = np.array(true_means.YPDAA)\n",
    "bs_YPDAA_lo = np.array(bs_low_all.YPDAA)\n",
    "bs_YPDAA_hi = np.array(bs_high_all.YPDAA)\n",
    "bs_YPDAA_CI = [bs_YPDAA_lo,bs_YPDAA_hi]\n",
    "########################################same for cume data\n",
    "#YPD_s30 data\n",
    "true_YPD_cum = np.array(true_cum.YPD)\n",
    "bs_YPD_lo_c = np.array(bs_low_cum.YPD)\n",
    "bs_YPD_hi_c = np.array(bs_high_cum.YPD)\n",
    "bs_YPD_CI_c = [bs_YPD_lo_c,bs_YPD_hi_c]\n",
    "#YPD_s37 data\n",
    "true_YPD37_cum = np.array(true_cum.YPD37)\n",
    "bs_YPD37_lo_c = np.array(bs_low_cum.YPD37)\n",
    "bs_YPD37_hi_c = np.array(bs_high_cum.YPD37)\n",
    "bs_YPD37_CI_c = [bs_YPD37_lo_c,bs_YPD37_hi_c]\n",
    "#YPD_s37H data\n",
    "true_YPD37H_cum = np.array(true_cum.YPD37H)\n",
    "bs_YPD37H_lo_c = np.array(bs_low_cum.YPD37H)\n",
    "bs_YPD37H_hi_c = np.array(bs_high_cum.YPD37H)\n",
    "bs_YPD37H_CI_c = [bs_YPD37H_lo_c,bs_YPD37H_hi_c]\n",
    "#YPD_sAA data\n",
    "true_YPDAA_cum = np.array(true_cum.YPDAA)\n",
    "bs_YPDAA_lo_c = np.array(bs_low_cum.YPDAA)\n",
    "bs_YPDAA_hi_c = np.array(bs_high_cum.YPDAA)\n",
    "bs_YPDAA_CI_c = [bs_YPDAA_lo_c,bs_YPDAA_hi_c]\n",
    "\n",
    "#setting up plot layout\n",
    "epoch = [\"0\",\"200\",\"400\",\"600\",\"800\",\"1000\"]\n",
    "cum = [\"all\"]\n",
    "epoch_array = np.array(epoch)\n",
    "cum_array = np.array(cum)\n",
    "#YPD\n",
    "axclus.errorbar(epoch_array,true_YPD_mean, linewidth=0.8, yerr=bs_YPD_CI, color='#2497FD', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD cumulative\n",
    "axclus.errorbar(cum_array, true_YPD_cum, linewidth=0.8, yerr=bs_YPD_CI_c, color='#2497FD', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD AA\n",
    "axclus.errorbar(epoch_array,true_YPDAA_mean, linewidth=0.8, yerr=bs_YPDAA_CI, color='#025F17', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD AA  cumulative\n",
    "axclus.errorbar(cum_array, true_YPDAA_cum, linewidth=0.8, yerr=bs_YPDAA_CI_c, color='#025F17', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD 37\n",
    "axclus.errorbar(epoch_array,true_YPD37_mean, linewidth=0.8, yerr=bs_YPD37_CI, color='#E1AB06', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD 37 cumulative\n",
    "axclus.errorbar(cum_array, true_YPD37_cum, linewidth=0.8, yerr=bs_YPD37_CI_c, color='#E1AB06', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD HAP\n",
    "axclus.errorbar(epoch_array,true_YPD37H_mean, linewidth=0.8, yerr=bs_YPD37H_CI, color='#D81B60', label=None,fmt='o',ms=5,linestyle='-')\n",
    "#YPD HAP cumulative\n",
    "axclus.errorbar(cum_array, true_YPD37H_cum, linewidth=0.8, yerr=bs_YPD37H_CI_c, color='#D81B60', label=None,fmt='o',ms=5,linestyle='-')\n",
    "\n",
    "axclus.set_ylabel('\\n'.join(wrap('Mean clustering coefficient',20)),size=8)\n",
    "axclus.set_xlabel(\"Generation\", size=8)\n",
    "axclus.tick_params(direction='out', length=3, width=1)\n",
    "plt.setp(axclus.get_xticklabels(), fontsize=7)\n",
    "plt.setp(axclus.get_yticklabels(), fontsize=7)\n",
    "axclus.set_ylim((0,5))\n",
    "\n",
    "#if removingoutlers == True:\n",
    "#    plt.savefig('fig6/PCA/4C_threshold_v01.pdf',bbox_inches='tight',dpi=300)\n",
    "#    plt.savefig('fig6/PCA/4C_threshold_v01.jpg',bbox_inches='tight',dpi=300)\n",
    "#else:\n",
    "#    plt.savefig('fig6/PCA/4C_unfiltered_v01.pdf',bbox_inches='tight',dpi=300)\n",
    "#    plt.savefig('fig6/PCA/4C_unfiltered_v01.jpg',bbox_inches='tight',dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 4 supplemental 2 and 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the datasets for supp 3\n",
    "\n",
    "# start with an evocode_list that is without YP10\n",
    "evocode_listupdate = new_evocode_list.copy(deep = True)\n",
    "evocode_listupdate['evoEnvt-ploidy'] = evocode_listupdate['evoEnvt']+\"-\"+evocode_listupdate['ploidy'].map(str)\n",
    "evocode_listupdate['evoEnvt-ploidy-LP'] = evocode_listupdate['evoEnvt-ploidy']+\"-\"+evocode_listupdate['LP'].map(str)\n",
    "# remove the YP10 guys\n",
    "tentest = evocode_listupdate['evoEnvt'] != 'YP10%D'\n",
    "evocode_listupdate = evocode_listupdate[tentest]\n",
    "evocode_listupdate = evocode_listupdate.reset_index(drop=True)\n",
    "\n",
    "# go through the evocode list and create a column for each fitness at each epoch (AA, NaCl, etc.). Basically,\n",
    "# just locate the evocode-epoch (e.g., p8-D3) and fill in the appropriate fitness values into each of 5 lists.\n",
    "\n",
    "endtable = evocode_listupdate\n",
    "\n",
    "for e in np.arange(len(epochind)):\n",
    "    YPDendlist = []\n",
    "    YPD37endlist = []\n",
    "    AAendlist = []\n",
    "    RTendlist = []\n",
    "    NaClendlist = []\n",
    "\n",
    "    for ec in np.arange(len(evocode_listupdate)):\n",
    "        picked = finaltable.loc[finaltable['evocode-Epoch'] == evocode_listupdate['evocode'][ec]+'-'+epochind['Epoch'][e]]\n",
    "        picked = picked.reset_index(drop=True)\n",
    "        YPDendlist = YPDendlist + [picked.loc[0,'s_30_adj']]\n",
    "        YPD37endlist = YPD37endlist + [picked.loc[0,'s_37_adj']]\n",
    "        AAendlist = AAendlist + [picked.loc[0,'s_AA_adj']]\n",
    "        RTendlist = RTendlist + [picked.loc[0,'s_RT_adj']]\n",
    "        NaClendlist = NaClendlist + [picked.loc[0,'s_NaCl_adj']]\n",
    "\n",
    "    endtable['30-'+epochind['Epoch'][e]] = YPDendlist\n",
    "    endtable['37-'+epochind['Epoch'][e]] = YPD37endlist\n",
    "    endtable['AA-'+epochind['Epoch'][e]] = AAendlist\n",
    "    endtable['RT-'+epochind['Epoch'][e]] = RTendlist\n",
    "    endtable['NaCl-'+epochind['Epoch'][e]] = NaClendlist\n",
    "    \n",
    "# now normalize this endtable data (standardization), since PCA output influenced by scale of data features.\n",
    "# ref: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
    "# Apply normalization with sklearn's StandardScaler. Want it to normalize such that it has a mean of 0, sd of 1,\n",
    "# Normally distributed.\n",
    "\n",
    "endfeatures = list(endtable)[6:len(list(endtable))]\n",
    "norm_endtable = endtable.loc[:,endfeatures].values\n",
    "norm_endtable = StandardScaler().fit_transform(norm_endtable)\n",
    "\n",
    "# now re-project data to n principle components\n",
    "npc_end = 2\n",
    "pca_end = PCA(n_components = npc_end)\n",
    "principalcomponents_end = pca_end.fit_transform(norm_endtable)\n",
    "\n",
    "# look at the variance explained\n",
    "#print('Explained variation per principal component: {}'.format(pca_end.explained_variance_ratio_))\n",
    "\n",
    "pcend_df = pd.DataFrame(data = principalcomponents_end\n",
    "                       , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "endtable['principal component 1'] = pcend_df['principal component 1']\n",
    "endtable['principal component 2'] = pcend_df['principal component 2']\n",
    "endtable['Epoch'] = 'all'\n",
    "\n",
    "# First, get the error bars (95% CIs) using bootstrapped s data\n",
    "\n",
    "PC1_boot = []\n",
    "PC2_boot = []\n",
    "PC1sum_boot = []\n",
    "PC2sum_boot = []\n",
    "\n",
    "nboots = 100\n",
    "\n",
    "for n in np.arange(nboots):\n",
    "    # start with an evocode_list that is without YP10\n",
    "    evocode_listupdate_boot = new_evocode_list.copy(deep = True)\n",
    "    evocode_listupdate_boot['evoEnvt-ploidy'] = evocode_listupdate_boot['evoEnvt']+\"-\"+evocode_listupdate_boot['ploidy'].map(str)\n",
    "    evocode_listupdate_boot['evoEnvt-ploidy-LP'] = evocode_listupdate_boot['evoEnvt-ploidy']+\"-\"+evocode_listupdate_boot['LP'].map(str)\n",
    "    \n",
    "    # remove the YP10 guys\n",
    "    tentest = evocode_listupdate_boot['evoEnvt'] != 'YP10%D'\n",
    "    evocode_listupdate_boot = evocode_listupdate_boot[tentest]\n",
    "    evocode_listupdate_boot = evocode_listupdate_boot.reset_index(drop=True)\n",
    "    \n",
    "    # Import bootstrapped s values\n",
    "    if removingoutliers == True:\n",
    "        boot_all = pd.read_csv(\"bootstraps/s/20200509_final_s_table_threshold_\"+str(n)+\"_wadj.csv\")\n",
    "    else:\n",
    "        boot_all = pd.read_csv(\"bootstraps/s/20200509_final_s_table_unfiltered_\"+str(n)+\"_wadj.csv\")\n",
    "    \n",
    "    endtable_boot = evocode_listupdate_boot.copy(deep=True)\n",
    "\n",
    "    for e in np.arange(len(epochind)):\n",
    "        YPDendlist = []\n",
    "        YPD37endlist = []\n",
    "        AAendlist = []\n",
    "        RTendlist = []\n",
    "        NaClendlist = []\n",
    "\n",
    "        for ec in np.arange(len(evocode_listupdate_boot)):\n",
    "            picked = boot_all.loc[boot_all['evocode-Epoch'] == evocode_listupdate_boot['evocode'][ec]+'-'+epochind['Epoch'][e]]\n",
    "            picked = picked.reset_index(drop=True)\n",
    "            YPDendlist = YPDendlist + [picked.loc[0,'s_30_adj']]\n",
    "            YPD37endlist = YPD37endlist + [picked.loc[0,'s_37_adj']]\n",
    "            AAendlist = AAendlist + [picked.loc[0,'s_AA_adj']]\n",
    "            RTendlist = RTendlist + [picked.loc[0,'s_RT_adj']]\n",
    "            NaClendlist = NaClendlist + [picked.loc[0,'s_NaCl_adj']]\n",
    "\n",
    "        endtable_boot['30-'+epochind['Epoch'][e]] = YPDendlist\n",
    "        endtable_boot['37-'+epochind['Epoch'][e]] = YPD37endlist\n",
    "        endtable_boot['AA-'+epochind['Epoch'][e]] = AAendlist\n",
    "        endtable_boot['RT-'+epochind['Epoch'][e]] = RTendlist\n",
    "        endtable_boot['NaCl-'+epochind['Epoch'][e]] = NaClendlist\n",
    "\n",
    "\n",
    "    # now normalize this endtable data (standardization), since PCA output influenced by scale of data features.\n",
    "    # ref: https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\n",
    "    # Apply normalization with sklearn's StandardScaler. Want it to normalize such that it has a mean of 0, sd of 1,\n",
    "    # Normally distributed.\n",
    "\n",
    "    endfeatures_boot = list(endtable_boot)[6:len(list(endtable_boot))]\n",
    "    norm_endtable_boot = endtable_boot.loc[:,endfeatures_boot].values\n",
    "    norm_endtable_boot = StandardScaler().fit_transform(norm_endtable_boot)\n",
    "\n",
    "    # now re-project data to 2 principle components\n",
    "    npc_end = 2\n",
    "    pca_end_boot = PCA(n_components = npc_end)\n",
    "    principalcomponents_end_boot = pca_end_boot.fit_transform(norm_endtable_boot)\n",
    "    \n",
    "    pceig_boot = pd.DataFrame(pca_end_boot.components_,columns=endfeatures_boot)\n",
    "    pceig_boot = pceig_boot.T\n",
    "    pceig_boot = pceig_boot.reset_index(drop=False)\n",
    "    pceig_boot = pceig_boot.rename(columns={\"index\":\"endfeature\",0:\"PC1\", 1:\"PC2\"})\n",
    "    pceig_boot['envt'] = \"\"\n",
    "    pceig_boot['epoch'] = \"\"\n",
    "    for i in np.arange(len(pceig_boot)):\n",
    "        pceig_boot.at[i,'envt'] = pceig_boot.loc[i,'endfeature'][0:pceig_boot.loc[i,'endfeature'].find('-')]\n",
    "        pceig_boot.at[i,'epoch'] = pceig_boot.loc[i,'endfeature'][pceig_boot.loc[i,'endfeature'].find('-')+1:len(pceig_boot.loc[i,'endfeature'])]\n",
    "    \n",
    "    pceig_boot['PC1plus'] = abs(pceig_boot['PC1'])\n",
    "    pceig_boot['PC2plus'] = abs(pceig_boot['PC2'])\n",
    "\n",
    "    sumplotter_boot = pd.DataFrame(columns=['Epoch','PC1sum','PC2sum'])\n",
    "    sumplotter_boot['Epoch'] = [0,200,400,600,800,1000]\n",
    "    for epoch in np.arange(len(epochind)):\n",
    "        sumplotter_boot.at[epoch,'PC1sum'] = sum(pceig_boot.loc[(pceig_boot['epoch'] == epochind['Epoch'][epoch])]['PC1plus'])\n",
    "        sumplotter_boot.at[epoch,'PC2sum'] = sum(pceig_boot.loc[(pceig_boot['epoch'] == epochind['Epoch'][epoch])]['PC2plus'])\n",
    "    sumplotter_boot['PC1sum'] = sumplotter_boot['PC1sum']/sum(sumplotter_boot['PC1sum'])\n",
    "    sumplotter_boot['PC2sum'] = sumplotter_boot['PC2sum']/sum(sumplotter_boot['PC2sum'])\n",
    "    \n",
    "    PC1_boot = PC1_boot + [pceig_boot['PC1'].values.tolist()]\n",
    "    PC2_boot = PC2_boot + [pceig_boot['PC2'].values.tolist()]\n",
    "    PC1sum_boot = PC1sum_boot + [sumplotter_boot['PC1sum'].values.tolist()]\n",
    "    PC2sum_boot = PC2sum_boot + [sumplotter_boot['PC2sum'].values.tolist()]\n",
    "    \n",
    "# For the bootstrapped datasets, integrate things into CI estimates\n",
    "PC1_boot_stats = []\n",
    "PC2_boot_stats = []\n",
    "PC1sum_boot_stats = []\n",
    "PC2sum_boot_stats = []\n",
    "\n",
    "for i in np.arange(len(PC1_boot[0])):\n",
    "    b1 = []\n",
    "    b2 = []\n",
    "    for n in np.arange(nboots):\n",
    "        b1 = b1 + [PC1_boot[n][i]]\n",
    "        b2 = b2 + [PC2_boot[n][i]]\n",
    "    ql1 = np.quantile(b1,0.025)\n",
    "    qu1 = np.quantile(b1,0.975)\n",
    "    mean1 = np.mean(b1)\n",
    "    ql2 = np.quantile(b2,0.025)\n",
    "    qu2 = np.quantile(b2,0.975)\n",
    "    mean2 = np.mean(b2)\n",
    "    PC1_boot_stats = PC1_boot_stats + [[ql1,qu1,mean1]]\n",
    "    PC2_boot_stats = PC2_boot_stats + [[ql2,qu2,mean2]]\n",
    "\n",
    "for j in np.arange(len(PC1sum_boot[0])):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    for n in np.arange(nboots):\n",
    "        s1 = s1 + [PC1sum_boot[n][j]]\n",
    "        s2 = s2 + [PC2sum_boot[n][j]]\n",
    "    sql1 = np.quantile(s1,0.025)\n",
    "    squ1 = np.quantile(s1,0.975)\n",
    "    smean1 = np.mean(s1)\n",
    "    sql2 = np.quantile(s2,0.025)\n",
    "    squ2 = np.quantile(s2,0.975)\n",
    "    smean2 = np.mean(s2)\n",
    "    PC1sum_boot_stats = PC1sum_boot_stats + [[sql1,squ1,smean1]]\n",
    "    PC2sum_boot_stats = PC2sum_boot_stats + [[sql2,squ2,smean2]]\n",
    "\n",
    "# each row corresponds to a feature (environment-epoch)\n",
    "PC1_boot_stats = pd.DataFrame(data=PC1_boot_stats,columns=['PC1-LQ','PC1-UQ','PC1-mean'])\n",
    "PC2_boot_stats = pd.DataFrame(data=PC2_boot_stats,columns=['PC2-LQ','PC2-UQ','PC2-mean'])\n",
    "# each row corresponds to a timepoint (epoch)\n",
    "PC1sum_boot_stats = pd.DataFrame(data=PC1sum_boot_stats,columns=['PC1-LQ','PC1-UQ','PC1-mean'])\n",
    "PC2sum_boot_stats = pd.DataFrame(data=PC2sum_boot_stats,columns=['PC2-LQ','PC2-UQ','PC2-mean'])\n",
    "\n",
    "# Now, extract the principal component eigenvalues in an intelligible way\n",
    "pceig = pd.DataFrame(pca_end.components_,columns=endfeatures)\n",
    "pceig = pceig.T\n",
    "pceig = pceig.reset_index(drop=False)\n",
    "pceig = pceig.rename(columns={\"index\":\"endfeature\",0:\"PC1\", 1:\"PC2\"})\n",
    "pceig['envt'] = \"\"\n",
    "pceig['epoch'] = \"\"\n",
    "for i in np.arange(len(pceig)):\n",
    "    pceig.at[i,'envt'] = pceig.loc[i,'endfeature'][0:pceig.loc[i,'endfeature'].find('-')]\n",
    "    pceig.at[i,'epoch'] = pceig.loc[i,'endfeature'][pceig.loc[i,'endfeature'].find('-')+1:len(pceig.loc[i,'endfeature'])]\n",
    "\n",
    "# Now plot the importance of each epoch to determining PCs over time\n",
    "pceig['PC1plus'] = abs(pceig['PC1'])\n",
    "pceig['PC2plus'] = abs(pceig['PC2'])\n",
    "\n",
    "sumplotter = pd.DataFrame(columns=['Epoch','PC1sum','PC2sum'])\n",
    "sumplotter['Epoch'] = [0,200,400,600,800,1000]\n",
    "for epoch in np.arange(len(epochind)):\n",
    "    sumplotter.at[epoch,'PC1sum'] = sum(pceig.loc[(pceig['epoch'] == epochind['Epoch'][epoch])]['PC1plus'])\n",
    "    sumplotter.at[epoch,'PC2sum'] = sum(pceig.loc[(pceig['epoch'] == epochind['Epoch'][epoch])]['PC2plus'])\n",
    "sumplotter['PC1sum'] = sumplotter['PC1sum']/sum(sumplotter['PC1sum'])\n",
    "sumplotter['PC2sum'] = sumplotter['PC2sum']/sum(sumplotter['PC2sum'])\n",
    "\n",
    "# Add the bootstrapped 95% CIs to the dataframes\n",
    "pceig = pceig.join(PC1_boot_stats[['PC1-LQ','PC1-UQ','PC1-mean']])\n",
    "pceig = pceig.join(PC2_boot_stats[['PC2-LQ','PC2-UQ','PC2-mean']])\n",
    "pceig['PC1-LE'] = pceig['PC1-mean'] - pceig['PC1-LQ']\n",
    "pceig['PC1-UE'] = pceig['PC1-UQ'] - pceig['PC1-mean']\n",
    "pceig['PC2-LE'] = pceig['PC2-mean'] - pceig['PC2-LQ']\n",
    "pceig['PC2-UE'] = pceig['PC2-UQ'] - pceig['PC2-mean']\n",
    "pceig['PC1meandiff'] = pceig['PC1'] - pceig['PC1-mean']\n",
    "pceig['PC2meandiff'] = pceig['PC2'] - pceig['PC2-mean']\n",
    "\n",
    "sumplotter = sumplotter.join(PC1sum_boot_stats[['PC1-LQ','PC1-UQ','PC1-mean']])\n",
    "sumplotter = sumplotter.join(PC2sum_boot_stats[['PC2-LQ','PC2-UQ','PC2-mean']])\n",
    "sumplotter['PC1-LE'] = sumplotter['PC1-mean'] - sumplotter['PC1-LQ']\n",
    "sumplotter['PC1-UE'] = sumplotter['PC1-UQ'] - sumplotter['PC1-mean']\n",
    "sumplotter['PC2-LE'] = sumplotter['PC2-mean'] - sumplotter['PC2-LQ']\n",
    "sumplotter['PC2-UE'] = sumplotter['PC2-UQ'] - sumplotter['PC2-mean']\n",
    "sumplotter['PC1meandiff'] = sumplotter['PC1sum'] - sumplotter['PC1-mean']\n",
    "sumplotter['PC2meandiff'] = sumplotter['PC2sum'] - sumplotter['PC2-mean']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now generate the dataset for supp 2\n",
    "\n",
    "# First, get the errorbars for plotting the PC contributions\n",
    "PC1conlist = []\n",
    "PC2conlist = []\n",
    "varlist = []\n",
    "\n",
    "nboots = 100\n",
    "\n",
    "for n in np.arange(nboots):\n",
    "    # start with an evocode_list that is without YP10\n",
    "    evocode_listupdate_boot = new_evocode_list.copy(deep = True)\n",
    "    evocode_listupdate_boot['evoEnvt-ploidy'] = evocode_listupdate_boot['evoEnvt']+\"-\"+evocode_listupdate_boot['ploidy'].map(str)\n",
    "    evocode_listupdate_boot['evoEnvt-ploidy-LP'] = evocode_listupdate_boot['evoEnvt-ploidy']+\"-\"+evocode_listupdate_boot['LP'].map(str)\n",
    "    # remove the YP10 guys\n",
    "    tentest = evocode_listupdate_boot['evoEnvt'] != 'YP10%D'\n",
    "    evocode_listupdate_boot = evocode_listupdate_boot[tentest]\n",
    "    evocode_listupdate_boot = evocode_listupdate_boot.reset_index(drop=True)\n",
    "    \n",
    "    if removingoutliers == True:\n",
    "        boot_all = pd.read_csv(\"bootstraps/s/20200509_final_s_table_threshold_\"+str(n)+\"_wadj.csv\")\n",
    "    else:\n",
    "        boot_all = pd.read_csv(\"bootstraps/s/20200509_final_s_table_unfiltered_\"+str(n)+\"_wadj.csv\")\n",
    "    \n",
    "    endtable_boot = evocode_listupdate_boot.copy(deep=True)\n",
    "\n",
    "    for e in np.arange(len(epochind)):\n",
    "        YPDendlist = []\n",
    "        YPD37endlist = []\n",
    "        AAendlist = []\n",
    "        RTendlist = []\n",
    "        NaClendlist = []\n",
    "\n",
    "        for ec in np.arange(len(evocode_listupdate)):\n",
    "            picked = boot_all.loc[boot_all['evocode-Epoch'] == evocode_listupdate_boot['evocode'][ec]+'-'+epochind['Epoch'][e]]\n",
    "            picked = picked.reset_index(drop=True)\n",
    "            YPDendlist = YPDendlist + [picked.loc[0,'s_30_adj']]\n",
    "            YPD37endlist = YPD37endlist + [picked.loc[0,'s_37_adj']]\n",
    "            AAendlist = AAendlist + [picked.loc[0,'s_AA_adj']]\n",
    "            RTendlist = RTendlist + [picked.loc[0,'s_RT_adj']]\n",
    "            NaClendlist = NaClendlist + [picked.loc[0,'s_NaCl_adj']]\n",
    "\n",
    "        endtable_boot['30-'+epochind['Epoch'][e]] = YPDendlist\n",
    "        endtable_boot['37-'+epochind['Epoch'][e]] = YPD37endlist\n",
    "        endtable_boot['AA-'+epochind['Epoch'][e]] = AAendlist\n",
    "        endtable_boot['RT-'+epochind['Epoch'][e]] = RTendlist\n",
    "        endtable_boot['NaCl-'+epochind['Epoch'][e]] = NaClendlist\n",
    "        \n",
    "    PC1cons = []\n",
    "    PC2cons = []\n",
    "    varl = []\n",
    "\n",
    "    for e in np.arange(len(epochind)):\n",
    "        specendtable_boot = endtable_boot[list(endtable_boot)[:6]+list(endtable_boot)[6+5*e:11+5*e]].copy(deep=True)\n",
    "\n",
    "        endfeatures = list(specendtable_boot)[6:len(list(specendtable_boot))]\n",
    "        norm_endtable_boot = specendtable_boot.loc[:,endfeatures].values\n",
    "        norm_endtable_boot = StandardScaler().fit_transform(norm_endtable_boot)\n",
    "\n",
    "        # now re-project data to n principle components\n",
    "        npc_end = 2\n",
    "        pca_end_boot = PCA(n_components = npc_end)\n",
    "        principalcomponents_end_boot = pca_end_boot.fit_transform(norm_endtable_boot)\n",
    "\n",
    "        pcend_df_boot = pd.DataFrame(data = principalcomponents_end_boot\n",
    "                               , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "        specendtable_boot['principal component 1'] = pcend_df_boot['principal component 1']\n",
    "        specendtable_boot['principal component 2'] = pcend_df_boot['principal component 2']\n",
    "        specendtable_boot['Epoch'] = float(epochind['Epoch'][e])\n",
    "\n",
    "        thesecomp_boot = pd.DataFrame(data=pca_end_boot.components_)\n",
    "        thesecomp_boot = thesecomp_boot.rename(columns={0:\"30\",1:\"37\", 2:\"AA\",3:\"RT\",4:\"NaCl\"})\n",
    "        thesecomp_boot = thesecomp_boot.T\n",
    "        thesecomp_boot = thesecomp_boot.rename(columns={0:'PC1',1:'PC2'})\n",
    "        thesecomp_boot['Epoch'] = float(epochind['Epoch'][e])\n",
    "        thesecomp_boot = thesecomp_boot.reset_index()\n",
    "        thesecomp_boot = thesecomp_boot.rename(columns={'index':'envt'})\n",
    "        \n",
    "        pca_boot = PCA().fit(norm_endtable_boot)\n",
    "        pca_boot_var = pca_boot.explained_variance_ratio_\n",
    "        thesevar_boot = pd.DataFrame(data=pca_boot_var)\n",
    "        thesevar_boot = thesevar_boot.rename(columns={0:'varexp'}).reset_index()\n",
    "        thesevar_boot = thesevar_boot.rename(columns={'index':'PC'})\n",
    "        thesevar_boot['Epoch'] = float(epochind['Epoch'][e])\n",
    "        \n",
    "        PC1cons = PC1cons + [thesecomp_boot['PC1'].values.tolist()]\n",
    "        PC2cons = PC2cons + [thesecomp_boot['PC2'].values.tolist()]\n",
    "        varl = varl + [thesevar_boot['varexp'].values.tolist()]\n",
    "    \n",
    "    PC1conlist = PC1conlist + [PC1cons]\n",
    "    PC2conlist = PC2conlist + [PC2cons]\n",
    "    varlist = varlist + [varl]\n",
    "\n",
    "# For each epoch in the bootstrapped list, go through and flip things to be same orientation, for reasonable error bars\n",
    "PC1conlist_work = PC1conlist\n",
    "PC2conlist_work = PC2conlist\n",
    "varlist_work = varlist\n",
    "\n",
    "for n in np.arange(len(PC1conlist_work)):\n",
    "    for e in np.arange(len(epochind)):\n",
    "        # Make 30 always positive for PC1\n",
    "        if PC1conlist_work[n][e][0] < 0:\n",
    "            PC1conlist_work[n][e] = [i * -1 for i in PC1conlist_work[n][e]]\n",
    "        \n",
    "        # Make AA always positive for PC2\n",
    "        if PC2conlist_work[n][e][2] < 0:\n",
    "            PC2conlist_work[n][e] = [i * -1 for i in PC2conlist_work[n][e]]\n",
    "\n",
    "# For the bootstrapped datasets, integrate into CI estimates\n",
    "LQUQ1 = []\n",
    "LQUQ2 = []\n",
    "LQUQv = []\n",
    "for e in np.arange(len(epochind)):\n",
    "    eLQUQ1 = []\n",
    "    eLQUQ2 = []\n",
    "    eLQUQv = []\n",
    "    for env in np.arange(len(envts)):\n",
    "        longlist1 = []\n",
    "        longlist2 = []\n",
    "        for n in np.arange(len(PC1conlist_work)):\n",
    "            longlist1 = longlist1 + [PC1conlist_work[n][e][env]]\n",
    "            longlist2 = longlist2 + [PC2conlist_work[n][e][env]]\n",
    "        ql1 = np.quantile(longlist1,0.025)\n",
    "        qu1 = np.quantile(longlist1,0.975)\n",
    "        mean1 = np.mean(longlist1)\n",
    "        ql2 = np.quantile(longlist2,0.025)\n",
    "        qu2 = np.quantile(longlist2,0.975)\n",
    "        mean2 = np.mean(longlist2)\n",
    "        eLQUQ1 = eLQUQ1 + [[ql1,qu1,mean1]]\n",
    "        eLQUQ2 = eLQUQ2 + [[ql2,qu2,mean2]]\n",
    "    LQUQ1 = LQUQ1 + [eLQUQ1]\n",
    "    LQUQ2 = LQUQ2 + [eLQUQ2]\n",
    "    \n",
    "    for var in np.arange(5):\n",
    "        longlistv = []\n",
    "        for n in np.arange(len(varlist_work)):\n",
    "            longlistv = longlistv + [varlist_work[n][e][var]]\n",
    "        qlv = np.quantile(longlistv,0.025)\n",
    "        quv = np.quantile(longlistv,0.975)\n",
    "        meanv = np.mean(longlistv)\n",
    "        eLQUQv = eLQUQv + [[qlv,quv,meanv]]\n",
    "    LQUQv = LQUQv + [eLQUQv]\n",
    "    \n",
    "LQUQt = [LQUQ1]+[LQUQ2]+[LQUQv]\n",
    "\n",
    "# make LQ, UQ in format that can be appended to eig_var later\n",
    "\n",
    "PC1holderL = pd.DataFrame(columns=['PC1-30LQ','PC1-37LQ','PC1-AALQ','PC1-RTLQ','PC1-NaClLQ'])\n",
    "PC2holderL = pd.DataFrame(columns=['PC2-30LQ','PC2-37LQ','PC2-AALQ','PC2-RTLQ','PC2-NaClLQ'])\n",
    "PC1holderU = pd.DataFrame(columns=['PC1-30UQ','PC1-37UQ','PC1-AAUQ','PC1-RTUQ','PC1-NaClUQ'])\n",
    "PC2holderU = pd.DataFrame(columns=['PC2-30UQ','PC2-37UQ','PC2-AAUQ','PC2-RTUQ','PC2-NaClUQ'])\n",
    "PC1holderM = pd.DataFrame(columns=['PC1-30m','PC1-37m','PC1-AAm','PC1-RTm','PC1-NaClm'])\n",
    "PC2holderM = pd.DataFrame(columns=['PC2-30m','PC2-37m','PC2-AAm','PC2-RTm','PC2-NaClm'])\n",
    "varholderL = pd.DataFrame(columns=['var1LQ','var2LQ','var3LQ','var4LQ','var5LQ'])\n",
    "varholderU = pd.DataFrame(columns=['var1UQ','var2UQ','var3UQ','var4UQ','var5UQ'])\n",
    "varholderM = pd.DataFrame(columns=['var1m','var2m','var3m','var4m','var5m'])\n",
    "\n",
    "vcolsL = ['var1LQ','var2LQ','var3LQ','var4LQ','var5LQ']\n",
    "vcolsU = ['var1UQ','var2UQ','var3UQ','var4UQ','var5UQ']\n",
    "vcolsM = ['var1m','var2m','var3m','var4m','var5m']\n",
    "pccols1L = ['PC1-30LQ','PC1-37LQ','PC1-AALQ','PC1-RTLQ','PC1-NaClLQ']\n",
    "pccols1U = ['PC1-30UQ','PC1-37UQ','PC1-AAUQ','PC1-RTUQ','PC1-NaClUQ']\n",
    "pccols1M = ['PC1-30m','PC1-37m','PC1-AAm','PC1-RTm','PC1-NaClm']\n",
    "pccols2L = ['PC2-30LQ','PC2-37LQ','PC2-AALQ','PC2-RTLQ','PC2-NaClLQ']\n",
    "pccols2U = ['PC2-30UQ','PC2-37UQ','PC2-AAUQ','PC2-RTUQ','PC2-NaClUQ']\n",
    "pccols2M = ['PC2-30m','PC2-37m','PC2-AAm','PC2-RTm','PC2-NaClm']\n",
    "\n",
    "\n",
    "for e in np.arange(len(epochind)):\n",
    "    for pc in np.arange(2):\n",
    "        if pc == 0:\n",
    "            PCL = pd.DataFrame(data=LQUQt[pc][e],index=pccols1L).T\n",
    "            PCL = PCL.iloc[0,:]\n",
    "            PC1holderL = PC1holderL.append(PCL[pccols1L]).reset_index(drop=True)\n",
    "            PCU = pd.DataFrame(data=LQUQt[pc][e],index=pccols1U).T\n",
    "            PCU = PCU.iloc[1,:]\n",
    "            PC1holderU = PC1holderU.append(PCU[pccols1U]).reset_index(drop=True)\n",
    "            PCm = pd.DataFrame(data=LQUQt[pc][e],index=pccols1M).T\n",
    "            PCm = PCm.iloc[2,:]\n",
    "            PC1holderM = PC1holderM.append(PCm[pccols1M]).reset_index(drop=True)\n",
    "        else:\n",
    "            PCL = pd.DataFrame(data=LQUQt[pc][e],index=pccols2L).T\n",
    "            PCL = PCL.iloc[0,:]\n",
    "            PC2holderL = PC2holderL.append(PCL[pccols2L]).reset_index(drop=True)\n",
    "            PCU = pd.DataFrame(data=LQUQt[pc][e],index=pccols2U).T\n",
    "            PCU = PCU.iloc[1,:]\n",
    "            PC2holderU = PC2holderU.append(PCU[pccols2U]).reset_index(drop=True)\n",
    "            PCm = pd.DataFrame(data=LQUQt[pc][e],index=pccols2M).T\n",
    "            PCm = PCm.iloc[2,:]\n",
    "            PC2holderM = PC2holderM.append(PCm[pccols2M]).reset_index(drop=True)\n",
    "    vL = pd.DataFrame(data=LQUQt[2][e],index=vcolsL).T\n",
    "    vL = vL.iloc[0,:]\n",
    "    varholderL = varholderL.append(vL[vcolsL]).reset_index(drop=True)\n",
    "    vU = pd.DataFrame(data=LQUQt[2][e],index=vcolsU).T\n",
    "    vU = vU.iloc[1,:]\n",
    "    varholderU = varholderU.append(vU[vcolsU]).reset_index(drop=True)\n",
    "    vm = pd.DataFrame(data=LQUQt[2][e],index=vcolsM).T\n",
    "    vm = vm.iloc[2,:]\n",
    "    varholderM = varholderM.append(vm[vcolsM]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do figure 4 supplements 2 and 3\n",
    "\n",
    "fig1 = plt.figure(constrained_layout=False,figsize=(4.8,3))\n",
    "gs1 = fig1.add_gridspec(1,2,wspace=1,hspace=0.3)\n",
    "\n",
    "# This sets up all the different parts of the figure\n",
    "gs10 = gs1[0,0].subgridspec(3,1)\n",
    "gs12 = gs1[0,1].subgridspec(3,1)\n",
    "\n",
    "aenames = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "envtcolors = ['#2497FD', '#025F17', '#E1AB06','#6B2E58','#A48DE2']\n",
    "\n",
    "axstats0 = fig1.add_subplot(gs10[0,0])\n",
    "for envt in np.arange(len(envts)):\n",
    "    axstats0.errorbar(list(epochind['Epoch'].astype(float)),\n",
    "                      PC1holderM['PC1-'+envts[envt]+'m'],\n",
    "                      yerr=[PC1holderM['PC1-'+envts[envt]+'m'] - PC1holderL['PC1-'+envts[envt]+'LQ'],\n",
    "                            PC1holderU['PC1-'+envts[envt]+'UQ'] - PC1holderM['PC1-'+envts[envt]+'m']],\n",
    "                      color=envtcolors[envt],marker='.',linestyle='dashed',linewidth=0.8)\n",
    "    axstats0.axhline(y=0,linewidth=0.75,color='xkcd:grey')\n",
    "\n",
    "axstats1 = fig1.add_subplot(gs10[1,0])\n",
    "for envt in np.arange(len(envts)):\n",
    "    axstats1.errorbar(list(epochind['Epoch'].astype(float)),\n",
    "                      PC2holderM['PC2-'+envts[envt]+'m'],\n",
    "                      yerr=[PC2holderM['PC2-'+envts[envt]+'m'] - PC2holderL['PC2-'+envts[envt]+'LQ'],\n",
    "                            PC2holderU['PC2-'+envts[envt]+'UQ'] - PC2holderM['PC2-'+envts[envt]+'m']],\n",
    "                      color=envtcolors[envt],marker='.',linestyle='dashed',linewidth=0.8)\n",
    "    axstats1.axhline(y=0,linewidth=0.75,color='xkcd:grey')\n",
    "\n",
    "axstats2 = fig1.add_subplot(gs10[2,0])\n",
    "markers = ['o','s','x','h','^']\n",
    "for i in np.arange(1,6):\n",
    "    axstats2.errorbar(list(epochind['Epoch'].astype(float)),varholderM['var'+str(i)+'m'],\n",
    "                      yerr=[varholderM['var'+str(i)+'m'] - varholderL['var'+str(i)+'LQ'],\n",
    "                            varholderU['var'+str(i)+'UQ'] - varholderM['var'+str(i)+'m']],\n",
    "                      marker=markers[i-1],markersize=3,color='xkcd:dark grey',\n",
    "                      #linestyle='dashed', # For v01\n",
    "                      linestyle='None', # For v02\n",
    "                      linewidth=0.8)\n",
    "\n",
    "axstats0.tick_params(axis='both',labelsize=7)\n",
    "axstats1.tick_params(axis='both',labelsize=7)\n",
    "axstats2.tick_params(axis='both',labelsize=7)\n",
    "\n",
    "axstats0.xaxis.set_ticks(np.arange(0,1200,200))\n",
    "axstats1.xaxis.set_ticks(np.arange(0,1200,200))\n",
    "axstats2.xaxis.set_ticks(np.arange(0,1200,200))\n",
    "\n",
    "axstats0.yaxis.set_ticks(np.arange(-.4,.7,.2))\n",
    "\n",
    "axstats0.axes.xaxis.set_visible(False)\n",
    "axstats1.axes.xaxis.set_visible(False)\n",
    "\n",
    "axstats0.yaxis.set_label_coords(-0.28, 0.5)\n",
    "axstats1.yaxis.set_label_coords(-0.28, 0.5)\n",
    "axstats2.yaxis.set_label_coords(-0.28, 0.5)\n",
    "\n",
    "rows = ['Contrib. to PC1','Contrib. to PC2','Var. exp. by PCs']\n",
    "rows = ['\\n'.join(wrap(l, 10)) for l in rows ]\n",
    "\n",
    "axstats0.set_ylabel(rows[0], rotation=90, size=8)\n",
    "axstats1.set_ylabel(rows[1], rotation=90, size=8)\n",
    "axstats2.set_ylabel(rows[2], rotation=90, size=8)\n",
    "\n",
    "axstats2.set_xlabel(\"Generation\", size=8)\n",
    "\n",
    "# Add legend\n",
    "mylw=2\n",
    "legend_el = [Line2D([0],[0],color=envtcolors[0],label=aenames[0],lw=mylw),\n",
    "            Line2D([0],[0],color=envtcolors[1],label=aenames[1],lw=mylw),\n",
    "            Line2D([0],[0],color=envtcolors[2],label=aenames[2],lw=mylw),\n",
    "            Line2D([0],[0],color=envtcolors[3],label=aenames[3],lw=mylw),\n",
    "            Line2D([0],[0],color=envtcolors[4],label=aenames[4],lw=mylw)]\n",
    "\n",
    "myms = 3\n",
    "legend_elC = [Line2D([0], [0], marker='o',color='xkcd:dark grey', label='PC1', markersize=myms,linestyle='None'),\n",
    "             Line2D([0], [0], marker='s',color='xkcd:dark grey', label='PC2', markersize=myms,linestyle='None'),\n",
    "             Line2D([0], [0], marker='x',color='xkcd:dark grey', label='PC3', markersize=myms,linestyle='None'),\n",
    "             Line2D([0], [0], marker='h',color='xkcd:dark grey', label='PC4', markersize=myms,linestyle='None'),\n",
    "             Line2D([0], [0], marker='^',color='xkcd:dark grey', label='PC5', markersize=myms,linestyle='None')]\n",
    "\n",
    "leg1 = fig1.legend(handles=legend_el,handlelength=1,fontsize=7,loc='upper center', ncol=5,borderaxespad=0,title='Assay environment')\n",
    "leg1._legend_box.align = \"left\"\n",
    "\n",
    "leg2 = axstats2.legend(handles=legend_elC,fontsize=7,loc='lower center',\n",
    "                       ncol=1,bbox_to_anchor=(1.2,-0.05),handletextpad=0,borderpad=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# ADD THE CUMULATIVE PCA STATS\n",
    "axcstats0 = fig1.add_subplot(gs12[0,0])\n",
    "for envt in np.arange(len(envts)):\n",
    "    temp = pceig.loc[(pceig['envt'] == envts[envt])].reset_index(drop=True)\n",
    "    axcstats0.errorbar(temp['epoch'],temp['PC1-mean'],yerr=[temp['PC1-LE'],temp['PC1-UE']],color=envtcolors[envt],marker='.',linewidth=0.8)\n",
    "    axcstats0.axhline(y=0,linewidth=0.75,color='xkcd:grey')\n",
    "\n",
    "axcstats1 = fig1.add_subplot(gs12[1,0])\n",
    "for envt in np.arange(len(envts)):\n",
    "    temp = pceig.loc[(pceig['envt'] == envts[envt])].reset_index(drop=True)\n",
    "    axcstats1.errorbar(temp['epoch'],temp['PC2-mean'],yerr=[temp['PC2-LE'],temp['PC2-UE']],color=envtcolors[envt],marker='.',linewidth=0.8)\n",
    "    axcstats1.axhline(y=0,linewidth=0.75,color='xkcd:grey')\n",
    "\n",
    "axcstats2 = fig1.add_subplot(gs12[2,0])\n",
    "axcstats2.errorbar(temp['epoch'],sumplotter['PC1-mean'],yerr=[sumplotter['PC1-LE'],sumplotter['PC1-UE']],color='xkcd:dark grey',marker='.',linewidth=0.8)\n",
    "axcstats2.errorbar(temp['epoch'],sumplotter['PC2-mean'],yerr=[sumplotter['PC2-LE'],sumplotter['PC2-UE']],color='xkcd:dark grey',marker='.',linestyle='dashed',linewidth=0.8)\n",
    "axcstats2.legend(['PC1','PC2'],loc='lower right',fontsize=7)\n",
    "\n",
    "axcstats0.tick_params(axis='both',labelsize=7)\n",
    "axcstats1.tick_params(axis='both',labelsize=7)\n",
    "axcstats2.tick_params(axis='both',labelsize=7)\n",
    "\n",
    "axcstats2.yaxis.set_ticks(np.arange(0,0.3,0.1))\n",
    "\n",
    "axcstats0.axes.xaxis.set_visible(False)\n",
    "axcstats1.axes.xaxis.set_visible(False)\n",
    "\n",
    "rows = ['Contrib. to PC1','Contrib. to PC2','Relative contrib.']\n",
    "rows = ['\\n'.join(wrap(l, 10)) for l in rows ]\n",
    "\n",
    "axcstats0.set_ylabel(rows[0], rotation=90, size=8)\n",
    "axcstats1.set_ylabel(rows[1], rotation=90, size=8)\n",
    "axcstats2.set_ylabel(rows[2], rotation=90, size=8)\n",
    "\n",
    "axcstats0.yaxis.set_label_coords(-0.28, 0.5)\n",
    "axcstats1.yaxis.set_label_coords(-0.28, 0.5)\n",
    "axcstats2.yaxis.set_label_coords(-0.28, 0.5)\n",
    "\n",
    "axcstats2.set_xlabel(\"Generation\", size=8)\n",
    "\n",
    "#if removingoutliers == True:\n",
    "#    fig1.savefig('fig6/PCA/fig4_b3rd_threshold_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "#else:\n",
    "#    fig1.savefig('fig6/PCA/fig4_b3rd_unfiltered_v02.pdf',bbox_inches='tight',dpi=3000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aenames = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "\n",
    "# populate the table in which we'll do this work\n",
    "nonmonsuper = pd.DataFrame(columns=['evocode','plate','evoEnvt','ploidy','assayEnvt',\n",
    "                                    's0','s0err','sl500','sl500err','sr500','sr500err',\n",
    "                                    'slfinal','slfinalerr','sfinal','sfinalerr'])\n",
    "\n",
    "myind = 0\n",
    "\n",
    "for ec in np.arange(len(new_evocode_list)):\n",
    "    subt = finaltable.loc[(finaltable['evocode'] == new_evocode_list.loc[ec,'evocode'])]\n",
    "    for e in np.arange(len(envts)):\n",
    "        nonmonsuper.at[myind,'evocode'] = new_evocode_list.loc[ec,'evocode']\n",
    "        nonmonsuper.at[myind,'plate'] = new_evocode_list.loc[ec,'evocode'][1]\n",
    "        nonmonsuper.at[myind,'evoEnvt'] = new_evocode_list.loc[ec,'evoEnvt']\n",
    "        nonmonsuper.at[myind,'ploidy'] = new_evocode_list.loc[ec,'ploidy']\n",
    "        nonmonsuper.at[myind,'assayEnvt'] = envts[e]\n",
    "        nonmonsuper.at[myind,'s0'] = subt.loc[(subt['Epoch'] == 0),'s_'+envts[e]].reset_index(drop=True)[0]\n",
    "        nonmonsuper.at[myind,'s0err'] = subt.loc[(subt['Epoch'] == 0),'stderr(s)_'+envts[e]].reset_index(drop=True)[0]          \n",
    "        nonmonsuper.at[myind,'sl500'] = subt.loc[(subt['Epoch'] == 400),'s_'+envts[e]].reset_index(drop=True)[0]\n",
    "        nonmonsuper.at[myind,'sl500err'] = subt.loc[(subt['Epoch'] == 400),'stderr(s)_'+envts[e]].reset_index(drop=True)[0]  \n",
    "        nonmonsuper.at[myind,'sr500'] = subt.loc[(subt['Epoch'] == 600),'s_'+envts[e]].reset_index(drop=True)[0]\n",
    "        nonmonsuper.at[myind,'sr500err'] = subt.loc[(subt['Epoch'] == 600),'stderr(s)_'+envts[e]].reset_index(drop=True)[0]  \n",
    "        nonmonsuper.at[myind,'slfinal'] = subt.loc[(subt['Epoch'] == 800),'s_'+envts[e]].reset_index(drop=True)[0]\n",
    "        nonmonsuper.at[myind,'slfinalerr'] = subt.loc[(subt['Epoch'] == 800),'stderr(s)_'+envts[e]].reset_index(drop=True)[0]  \n",
    "        nonmonsuper.at[myind,'sfinal'] = subt.loc[(subt['Epoch'] == 1000),'s_'+envts[e]].reset_index(drop=True)[0]\n",
    "        nonmonsuper.at[myind,'sfinalerr'] = subt.loc[(subt['Epoch'] == 1000),'stderr(s)_'+envts[e]].reset_index(drop=True)[0]\n",
    "        \n",
    "        myind = myind + 1\n",
    "\n",
    "# remove the references\n",
    "nonmonsuper = nonmonsuper.loc[(nonmonsuper['evoEnvt'] != \"YP10%D\")].reset_index(drop=True)\n",
    "\n",
    "# do the easy averaging of epoch 400 and 600 to get gen 500 estimate for all that aren't plate 2\n",
    "nonmonsuperno2 = nonmonsuper.loc[(nonmonsuper['plate'] != '2')].reset_index(drop=True)\n",
    "nonmonsuperno2['s500'] = nonmonsuperno2[['sl500','sr500']].mean(axis=1)\n",
    "nonmonsuperno2['s500err'] = \"\"\n",
    "nonmonsuperno2['s500err'] = np.sqrt((nonmonsuperno2['sl500err']**2+nonmonsuperno2['sr500err']**2).astype(float))/2\n",
    "nonmonsuperno2['s1000'] = nonmonsuperno2['sfinal']\n",
    "nonmonsuperno2['s1000err'] = nonmonsuperno2['sfinalerr']\n",
    "\n",
    "# Now do the harder interpolation and extrapolation for plate 2 guys\n",
    "# Try to do this with weighted averages for the errors -- should be equivalent\n",
    "# to the interpolation math. Do extrapolation separately\n",
    "nonmonsuper2 = nonmonsuper.loc[(nonmonsuper['plate'] == '2')].reset_index(drop=True)\n",
    "nonmonsuper2['s500'] = ''\n",
    "nonmonsuper2['s500err'] = ''\n",
    "nonmonsuper2['s1000'] = ''\n",
    "nonmonsuper2['s1000err'] = ''\n",
    "\n",
    "for i in np.arange(len(nonmonsuper2)):\n",
    "    # interpolation\n",
    "    my_m = (nonmonsuper2.loc[i,'sr500'] - nonmonsuper2.loc[i,'sl500'])/200\n",
    "    my_b = nonmonsuper2.loc[i,'sl500'] - my_m * 360\n",
    "    nonmonsuper2.at[i,'s500'] = my_m * 500 + my_b\n",
    "    \n",
    "    # for the error, use a different approach, where calculating the midway point is done by multiplying\n",
    "    # y1 and y2 by two weights respectively (each 1/[abs(x3-x1)/abs(x2-x1)] and 1/[abs(x3-x2)/abs(x2-x1)], respectively).\n",
    "    # Then divide the sum of those by the sum of the weights. \n",
    "    # The weights have no error associated with them.\n",
    "    \n",
    "    wt1 = 1/(abs(500-360)/abs(560-360))\n",
    "    wt2 = 1/(abs(500-560)/abs(560-360))\n",
    "    nonmonsuper2.at[i,'s500err'] = np.sqrt((wt1*nonmonsuper2.loc[i,'sl500err'])**2+(wt2*nonmonsuper2.loc[i,'sr500err'])**2)/(wt1+wt2)\n",
    "    \n",
    "    #extrapolation\n",
    "    my_m = (nonmonsuper2.loc[i,'sfinal'] - nonmonsuper2.loc[i,'slfinal'])/200\n",
    "    my_b = nonmonsuper2.loc[i,'slfinal'] - my_m * 760\n",
    "    nonmonsuper2.at[i,'s1000'] = my_m * 1000 + my_b\n",
    "    # For the error, just take the error on the final timepoint we have\n",
    "    nonmonsuper2['s1000err'] = nonmonsuper2['sfinalerr']\n",
    "    \n",
    "nonmonsuper = nonmonsuperno2.append(nonmonsuper2).reset_index(drop=True)\n",
    "\n",
    "nonmonsuper['ds_0-500'] = nonmonsuper['s500'] - nonmonsuper['s0']\n",
    "nonmonsuper['ds_500-1000'] = nonmonsuper['s1000'] - nonmonsuper['s500']\n",
    "nonmonsuper['ds_0-500err'] = np.sqrt((nonmonsuper['s0err']**2 + nonmonsuper['s500err']**2).astype(float))\n",
    "nonmonsuper['ds_500-1000err'] = np.sqrt((nonmonsuper['s500err']**2 + nonmonsuper['s1000err']**2).astype(float))\n",
    "\n",
    "\n",
    "# catalog how many in each quadrant for each evolution environment, assay environment, and how ambiguous or not\n",
    "stable = pd.DataFrame()\n",
    "stable['evoEnvt'] = \"\"\n",
    "stable['ploidy'] = \"\"\n",
    "stable['assayEnvt'] = \"\"\n",
    "for q in np.arange(4):\n",
    "    stable['Q'+str(q+1)+'-clearcount'] = \"\"\n",
    "    stable['Q'+str(q+1)+'-ambcount'] = \"\"\n",
    "\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for ee in np.arange(len(evoenvsno10)):\n",
    "        temper = nonmonsuper.loc[(nonmonsuper['ploidy'] == ploidies[p]) & (nonmonsuper['evoEnvt'] == evoenvs[ee])].reset_index(drop=True)\n",
    "        if len(temper) > 0:\n",
    "            for ae in np.arange(len(envts)):\n",
    "                t2 = temper.loc[(temper['assayEnvt'] == envts[ae])].reset_index(drop=True)\n",
    "                q1clear = 0\n",
    "                q2clear = 0\n",
    "                q3clear = 0\n",
    "                q4clear = 0\n",
    "                q1amb = 0\n",
    "                q2amb = 0\n",
    "                q3amb = 0\n",
    "                q4amb = 0\n",
    "                for i in np.arange(len(t2)):\n",
    "                    xco = t2.loc[i,'ds_0-500']\n",
    "                    xe = t2.loc[i,'ds_0-500err']\n",
    "                    yco = t2.loc[i,'ds_500-1000']\n",
    "                    ye = t2.loc[i,'ds_500-1000err']\n",
    "                    if xco < 0:\n",
    "                        if yco < 0:\n",
    "                            if xco + xe >= 0 or yco + ye >= 0:\n",
    "                                q3amb = q3amb + 1\n",
    "                            else:\n",
    "                                q3clear = q3clear + 1\n",
    "                        else:\n",
    "                            if xco + xe >= 0 or yco - ye <= 0:\n",
    "                                q2amb = q2amb + 1\n",
    "                            else:\n",
    "                                q2clear = q2clear + 1\n",
    "                    else:\n",
    "                        if yco < 0:\n",
    "                            if xco - xe <= 0 or yco + ye >= 0:\n",
    "                                q4amb = q4amb + 1\n",
    "                            else:\n",
    "                                q4clear = q4clear + 1\n",
    "                        else:\n",
    "                            if xco - xe <= 0 or yco - ye <= 0:\n",
    "                                q1amb = q1amb + 1\n",
    "                            else:\n",
    "                                q1clear = q1clear + 1\n",
    "                stableadd = pd.DataFrame()\n",
    "                stableadd['evoEnvt'] = \"\"\n",
    "                stableadd['ploidy'] = \"\"\n",
    "                stableadd['assayEnvt'] = \"\"\n",
    "                for q in np.arange(4):\n",
    "                    stableadd['Q'+str(q+1)+'-clearcount'] = \"\"\n",
    "                    stableadd['Q'+str(q+1)+'-ambcount'] = \"\"\n",
    "                stableadd.at[0,'evoEnvt'] = evoenvsno10[ee]\n",
    "                stableadd.at[0,'ploidy'] = ploidies[p]\n",
    "                stableadd.at[0,'assayEnvt'] = envts[ae]\n",
    "                stableadd.at[0,'Q1-clearcount'] = q1clear\n",
    "                stableadd.at[0,'Q1-ambcount'] = q1amb\n",
    "                stableadd.at[0,'Q2-clearcount'] = q2clear\n",
    "                stableadd.at[0,'Q2-ambcount'] = q2amb\n",
    "                stableadd.at[0,'Q3-clearcount'] = q3clear\n",
    "                stableadd.at[0,'Q3-ambcount'] = q3amb\n",
    "                stableadd.at[0,'Q4-clearcount'] = q4clear\n",
    "                stableadd.at[0,'Q4-ambcount'] = q4amb\n",
    "                stable = stable.append(stableadd).reset_index(drop=True)\n",
    "\n",
    "stable['clear1-3'] = stable['Q1-clearcount'] + stable['Q3-clearcount']\n",
    "stable['amb1-3'] = stable['Q1-ambcount'] + stable['Q3-ambcount']\n",
    "stable['clear2-4'] = stable['Q2-clearcount'] + stable['Q4-clearcount']\n",
    "stable['amb2-4'] = stable['Q2-ambcount'] + stable['Q4-ambcount']\n",
    "stable['total'] = stable['clear1-3'] + stable['amb1-3'] + stable['clear2-4'] + stable['amb2-4']\n",
    "\n",
    "\n",
    "alphaless = 0.5\n",
    "alphamore = 0.95\n",
    "\n",
    "\n",
    "# Let's see if we can combine these 2 sub-figures into a bigger FIGURE 5\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(7,3), gridspec_kw={'height_ratios': [2,1]},\n",
    "                         sharex=False, sharey='row')#,constrained_layout=True)\n",
    "\n",
    "#eenames = ['YPD, n=43','YPD + Acetic acid, n=48','YPD, 37°C, diploid, n=42','YPD, 37°C, haploid, n=20']\n",
    "eenames = ['YPD','YPD + Acetic acid','YPD, 37°C (dip.)','YPD, 37°C (hap.)']\n",
    "eecolors = ['#2497FD', '#025F17', '#E1AB06', '#D81B60']\n",
    "eenames = ['\\n'.join(wrap(l, 18)) for l in eenames ]\n",
    "\n",
    "aecolors = ['#2497FD', '#025F17', '#E1AB06','#6B2E58','#A48DE2']\n",
    "\n",
    "for ax, col in zip(axes[0,:], eenames):\n",
    "    ax.set_title(col, size=8,fontweight='bold')\n",
    "\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for ee in np.arange(len(evoenvs)-1):\n",
    "        temper = nonmonsuper.loc[(nonmonsuper['ploidy'] == ploidies[p]) & (nonmonsuper['evoEnvt'] == evoenvs[ee])].reset_index(drop=True)\n",
    "        if len(temper) > 0:\n",
    "            if p > 0:\n",
    "                for ae in np.arange(len(envts)):\n",
    "                    t2 = temper.loc[(temper['assayEnvt'] == envts[ae])]\n",
    "                    if ae != ee:\n",
    "                        axes[0,ee].errorbar(t2['ds_0-500'],t2['ds_500-1000'],xerr=t2['ds_0-500err'],yerr=t2['ds_500-1000err'],\n",
    "                                      c=aecolors[ae],alpha=alphaless,marker='o',linestyle='None',ms=0,elinewidth=0.6)\n",
    "            else:\n",
    "                for ae in np.arange(len(envts)):\n",
    "                    t2 = temper.loc[(temper['assayEnvt'] == envts[ae])]\n",
    "                    if ae != ee:\n",
    "                        axes[0,ee+1].errorbar(t2['ds_0-500'],t2['ds_500-1000'],xerr=t2['ds_0-500err'],yerr=t2['ds_500-1000err'],\n",
    "                                            c=aecolors[ae],alpha=alphaless,marker='o',linestyle='None',ms=0,elinewidth=0.6)\n",
    "            \n",
    "# now overlay the home environment ones\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for ee in np.arange(len(evoenvs)-1):\n",
    "        temper = nonmonsuper.loc[(nonmonsuper['ploidy'] == ploidies[p]) & (nonmonsuper['evoEnvt'] == evoenvs[ee])].reset_index(drop=True)\n",
    "        if len(temper) > 0:\n",
    "            if p > 0:\n",
    "                for ae in np.arange(len(envts)):\n",
    "                    t2 = temper.loc[(temper['assayEnvt'] == envts[ae])]\n",
    "                    if ae == ee:\n",
    "                        axes[0,ee].errorbar(t2['ds_0-500'],t2['ds_500-1000'],xerr=t2['ds_0-500err'],yerr=t2['ds_500-1000err'],\n",
    "                                      c=aecolors[ae],alpha=alphamore,marker='o',linestyle='None',ms=0,elinewidth=0.6)\n",
    "            else:\n",
    "                for ae in np.arange(len(envts)):\n",
    "                    t2 = temper.loc[(temper['assayEnvt'] == envts[ae])]\n",
    "                    if ae == ee:\n",
    "                        axes[0,ee+1].errorbar(t2['ds_0-500'],t2['ds_500-1000'],xerr=t2['ds_0-500err'],yerr=t2['ds_500-1000err'],\n",
    "                                            c=aecolors[ae],alpha=alphamore,marker='o',linestyle='None',ms=0,elinewidth=0.6)                        \n",
    "\n",
    "for ec in np.arange(4):\n",
    "    axes[0,ec].tick_params(axis='both',labelsize=7)\n",
    "    axes[1,ec].tick_params(axis='both',labelsize=7)\n",
    "    axes[0,ec].axhline(y=0,linewidth=0.3,color='xkcd:grey',alpha=1,zorder=1)\n",
    "    axes[0,ec].axvline(x=0,linewidth=0.3,color='xkcd:grey',alpha=1,zorder=1)\n",
    "        \n",
    "#fig.text(0.56, -0.03, '% trajectories non-monotonic', ha='center',size=8)\n",
    "\n",
    "#axes[0].set_ylabel('Assay environment',fontsize=8)\n",
    "\n",
    "fig.suptitle('Evolution environment',x=.51,y=1.005,size=10,fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "plt.rcParams['legend.title_fontsize'] = 7\n",
    "aenames = ['YPD','YPD + Acetic acid','YPD, 37°C','YPD, 21°C','YPD + NaCl']\n",
    "mylw=0.7\n",
    "mymew=0.7\n",
    "legend_el = [Line2D([0],[0],color=aecolors[0],label=aenames[0],lw=mylw,marker='+',markeredgewidth=mymew,linestyle='',alpha=0.8),\n",
    "            Line2D([0],[0],color=aecolors[1],label=aenames[1],lw=mylw,marker='+',markeredgewidth=mymew,linestyle='',alpha=0.8),\n",
    "            Line2D([0],[0],color=aecolors[2],label=aenames[2],lw=mylw,marker='+',markeredgewidth=mymew,linestyle='',alpha=0.8),\n",
    "            Line2D([0],[0],color=aecolors[3],label=aenames[3],lw=mylw,marker='+',markeredgewidth=mymew,linestyle='',alpha=0.8),\n",
    "            Line2D([0],[0],color=aecolors[4],label=aenames[4],lw=mylw,marker='+',markeredgewidth=mymew,linestyle='',alpha=0.8)]\n",
    "\n",
    "leg1 = fig.legend(handles=legend_el,handlelength=1,fontsize=7,ncol=5,columnspacing=1,handletextpad=0.5,\n",
    "                  loc='upper left',title='Assay environment',bbox_to_anchor=(0.05, 1.17))\n",
    "leg1._legend_box.align = \"left\"\n",
    "\n",
    "fig.text(0.51, .38, '∆ Fitness, 0-500 gen.', ha='center',size=8,fontweight='bold')\n",
    "\n",
    "axes[0,0].set_ylabel('\\n'.join(wrap('∆ Fitness, 500-1000 gen.',15)),fontsize=8,fontweight='bold')\n",
    "\n",
    "for ee in np.arange(4):\n",
    "    if ee == 3:\n",
    "        plt.setp(axes[0,ee].spines.values(),color=eecolors[ee-1],lw=1.25,alpha=0.7)\n",
    "        plt.setp([axes[0,ee].get_xticklines(),axes[0,ee].get_yticklines()],color=eecolors[ee-1])\n",
    "    else:\n",
    "        plt.setp(axes[0,ee].spines.values(),color=eecolors[ee],lw=1.25,alpha=0.7)\n",
    "        plt.setp([axes[0,ee].get_xticklines(),axes[0,ee].get_yticklines()],color=eecolors[ee])\n",
    "    \n",
    "    \n",
    "thisalpha = 0.2\n",
    "for i in np.arange(4):\n",
    "    xmin = axes[0,i].get_xlim()[0]\n",
    "    xmax = axes[0,i].get_xlim()[1]\n",
    "    ymin = axes[0,i].get_ylim()[0]\n",
    "    ymax = axes[0,i].get_ylim()[1]\n",
    "    axes[0,i].autoscale(False)\n",
    "    axes[0,i].fill([xmin,0,0,xmin], [0,0,ymax,ymax], color='xkcd:light grey', alpha=thisalpha, edgecolor=None, zorder=0)\n",
    "    axes[0,i].fill([0,xmax,xmax,0], [ymin,ymin,0,0], color='xkcd:light grey', alpha=thisalpha, edgecolor=None, zorder=0)\n",
    "\n",
    "\n",
    "# Now insert the bar charts\n",
    "tempertotal = pd.DataFrame()\n",
    "for p in np.arange(len(ploidies)):\n",
    "    for ee in np.arange(len(evoenvsno10)):\n",
    "        temper = stable.loc[(stable['ploidy'] == ploidies[p]) & (stable['evoEnvt'] == evoenvs[ee])].reset_index(drop=True)\n",
    "        if len(temper) > 0:\n",
    "            for ae in np.arange(len(envts)):\n",
    "                temper2 = temper.loc[(temper['assayEnvt'] == envts[ae])].reset_index(drop=True)\n",
    "                if p > 0:\n",
    "                    if evoenvs_simp[ee] == envts[ae]:\n",
    "                        axes[1,ee].barh(envts_abbrevofficial[ae],(temper2['clear2-4']/temper2['total']*100), color = aecolors[ae],alpha=alphamore)\n",
    "                        axes[1,ee].xaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "                    else:\n",
    "                        axes[1,ee].barh(envts_abbrevofficial[ae],(temper2['clear2-4']/temper2['total']*100), color = aecolors[ae],alpha=alphamore)\n",
    "                        axes[1,ee].xaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "                else:\n",
    "                    if evoenvs_simp[ee] == envts[ae]:\n",
    "                        axes[1,ee+1].barh(envts_abbrevofficial[ae],(temper2['clear2-4']/temper2['total']*100), color = aecolors[ae],alpha=alphamore)\n",
    "                        axes[1,ee+1].xaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))\n",
    "                    else:\n",
    "                        axes[1,ee+1].barh(envts_abbrevofficial[ae],(temper2['clear2-4']/temper2['total']*100), color = aecolors[ae],alpha=alphamore)\n",
    "                        axes[1,ee+1].xaxis.set_major_formatter(mtick.PercentFormatter(decimals=0))            \n",
    "                \n",
    "fig.gca().invert_yaxis()\n",
    "\n",
    "fig.text(0.51, 0.025, '% trajectories non-monotonic', ha='center',size=8,fontweight='bold')\n",
    "\n",
    "axes[1,0].set_ylabel('\\n'.join(wrap('Assay environment',15)),fontsize=8,fontweight='bold')\n",
    "\n",
    "for ee in np.arange(4):\n",
    "    if ee == 3:\n",
    "        plt.setp(axes[1,ee].spines.values(),color=eecolors[ee-1],lw=1.25,alpha=0.7)\n",
    "        plt.setp([axes[1,ee].get_xticklines(),axes[1,ee].get_yticklines()],color=eecolors[ee-1])\n",
    "    else:\n",
    "        plt.setp(axes[1,ee].spines.values(),color=eecolors[ee],lw=1.25,alpha=0.7)\n",
    "        plt.setp([axes[1,ee].get_xticklines(),axes[1,ee].get_yticklines()],color=eecolors[ee])\n",
    "    axes[1,ee].set_xlim([0, 50])\n",
    "\n",
    "plt.subplots_adjust(hspace=.5)\n",
    "\n",
    "fig.align_ylabels(axes[:, 0])\n",
    "\n",
    "fig.text(0.045,.95,\"A\",ha='center',size=11,fontweight='bold')\n",
    "fig.text(0.045,.38,\"B\",ha='center',size=11,fontweight='bold')\n",
    "\n",
    "#if removingoutliers == True:\n",
    "    #plt.savefig('fignonmon/composite_threshold_v09.pdf',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fignonmon/composite_threshold_v09.jpg',bbox_inches='tight',dpi=3000)\n",
    "#else:\n",
    "    #plt.savefig('fignonmon/composite_unfiltered_v09.pdf',bbox_inches='tight',dpi=3000)\n",
    "    #plt.savefig('fignonmon/composite_unfiltered_v09.jpg',bbox_inches='tight',dpi=3000)\n",
    "    \n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
